{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands Agents with AWS Neptune MCP Server Demo\n",
    "\n",
    "This notebook demonstrates how to use [Strands Agents](https://strandsagents.com/) with the [AWS Neptune MCP Server](https://awslabs.github.io/mcp/servers/amazon-neptune-mcp-server/) to intelligently interact with Amazon Neptune graphs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This demo showcases:\n",
    "1. **Neptune MCP Server Setup**: Connect to Neptune Analytics or Neptune Database\n",
    "2. **Graph Status Monitoring**: Check if the graph is available\n",
    "3. **Schema Discovery**: Retrieve and analyze graph structure\n",
    "4. **Intelligent Querying**: Use Strands Agents to generate and execute openCypher queries\n",
    "5. **Autonomous Graph Analysis**: Let the agent explore and analyze graph data\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Prompt ‚Üí Strands Agent ‚Üí Neptune MCP Server ‚Üí Neptune Graph\n",
    "                    ‚Üì\n",
    "              Amazon Bedrock\n",
    "              (Claude 3.5 Sonnet)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# For Neptune Analytics, use format: neptune-graph://<graph-identifier>\n",
    "# For Neptune Database, use format: neptune-db://<cluster-endpoint>\n",
    "\n",
    "NEPTUNE_ENDPOINT = \"neptune-graph://<INPUT_GRAPH_ID>\"\n",
    "AWS_REGION = \"us-west-2\"\n",
    "AWS_PROFILE = None\n",
    "\n",
    "# Bedrock configuration\n",
    "BEDROCK_MODEL = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "BEDROCK_REGION = \"us-west-2\"\n",
    "\n",
    "# Set environment variables for Neptune MCP server\n",
    "os.environ['NEPTUNE_ENDPOINT'] = NEPTUNE_ENDPOINT\n",
    "os.environ['AWS_REGION'] = AWS_REGION\n",
    "if AWS_PROFILE:\n",
    "    os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"  Neptune Endpoint: {NEPTUNE_ENDPOINT}\")\n",
    "print(f\"  AWS Region: {AWS_REGION}\")\n",
    "print(f\"  Bedrock Model: {BEDROCK_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Neptune MCP Client\n",
    "\n",
    "The Neptune MCP server provides three main tools:\n",
    "- `get_status()`: Check if the graph is available\n",
    "- `get_schema()`: Retrieve the graph schema\n",
    "- `run_query()`: Execute openCypher or Gremlin queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neptune MCP client creator ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize Neptune MCP Client\n",
    "def create_neptune_mcp_client():\n",
    "    \"\"\"Create and return a Neptune MCP client.\"\"\"\n",
    "    return MCPClient(lambda: stdio_client(\n",
    "        StdioServerParameters(\n",
    "            command=\"uvx\",\n",
    "            args=[\"awslabs.amazon-neptune-mcp-server@latest\"],\n",
    "            env={\n",
    "                \"NEPTUNE_ENDPOINT\": NEPTUNE_ENDPOINT,\n",
    "                \"AWS_REGION\": AWS_REGION,\n",
    "                \"FASTMCP_LOG_LEVEL\": \"INFO\"\n",
    "            }\n",
    "        )\n",
    "    ))\n",
    "\n",
    "print(\"Neptune MCP client creator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Direct Neptune MCP Operations\n",
    "\n",
    "Let's test basic Neptune operations using the MCP server directly.\n",
    "reference: https://strandsagents.com/latest/documentation/docs/api-reference/tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Neptune MCP Server...\n",
      "\n",
      "Available Neptune MCP Tools: 4\n",
      "  - get_graph_status: {'inputSchema': {'json': {'properties': {}, 'title': 'get_statusArguments', 'type': 'object'}}, 'name': 'get_graph_status', 'description': 'Get the status of the currently configured Amazon Neptune graph.', 'outputSchema': {'json': {'properties': {'result': {'title': 'Result', 'type': 'string'}}, 'required': ['result'], 'title': 'get_statusOutput', 'type': 'object'}}}\n",
      "  - get_graph_schema: {'inputSchema': {'json': {'properties': {}, 'title': 'get_schemaArguments', 'type': 'object'}}, 'name': 'get_graph_schema', 'description': 'Get the schema for the graph including the vertex and edge labels as well as the\\n    (vertex)-[edge]->(vertex) combinations.\\n    ', 'outputSchema': {'json': {'$defs': {'Node': {'description': 'Defines a node type in the graph schema.\\n\\nNodes represent entities in the graph database and can have labels\\nand properties that describe their characteristics.\\n\\nAttributes:\\n    labels (str): The label(s) that categorize this node type\\n    properties (List[Property]): List of properties that can be assigned to this node type', 'properties': {'labels': {'title': 'Labels', 'type': 'string'}, 'properties': {'type': 'array', 'items': {'$ref': '#/$defs/Property'}, 'default': [], 'title': 'Properties'}}, 'required': ['labels'], 'title': 'Node', 'type': 'object'}, 'Property': {'description': 'Represents a property definition for nodes and relationships in the graph.\\n\\nProperties are key-value pairs that can be attached to both nodes and\\nrelationships, storing additional metadata about these graph elements.\\n\\nAttributes:\\n    name (str): The name/key of the property\\n    type (str): The data type of the property value', 'properties': {'name': {'title': 'Name', 'type': 'string'}, 'type': {'items': {'type': 'string'}, 'title': 'Type', 'type': 'array'}}, 'required': ['name', 'type'], 'title': 'Property', 'type': 'object'}, 'Relationship': {'description': 'Defines a relationship type in the graph schema.\\n\\nRelationships represent connections between nodes in the graph and can\\nhave their own properties to describe the nature of the connection.\\n\\nAttributes:\\n    type (str): The type/category of the relationship\\n    properties (List[Property]): List of properties that can be assigned to this relationship type', 'properties': {'type': {'title': 'Type', 'type': 'string'}, 'properties': {'type': 'array', 'items': {'$ref': '#/$defs/Property'}, 'default': [], 'title': 'Properties'}}, 'required': ['type'], 'title': 'Relationship', 'type': 'object'}, 'RelationshipPattern': {'description': 'Defines a valid relationship pattern between nodes in the graph.\\n\\nRelationship patterns describe the allowed connections between different\\ntypes of nodes in the graph schema.\\n\\nAttributes:\\n    left_node (str): The label of the source/starting node\\n    right_node (str): The label of the target/ending node\\n    relation (str): The type of relationship connecting the nodes', 'properties': {'left_node': {'title': 'Left Node', 'type': 'string'}, 'right_node': {'title': 'Right Node', 'type': 'string'}, 'relation': {'title': 'Relation', 'type': 'string'}}, 'required': ['left_node', 'right_node', 'relation'], 'title': 'RelationshipPattern', 'type': 'object'}}, 'description': 'Represents the complete schema definition for the graph database.\\n\\nThe graph schema defines all possible node types, relationship types,\\nand valid patterns of connections between nodes.\\n\\nAttributes:\\n    nodes (List[Node]): List of all node types defined in the schema\\n    relationships (List[Relationship]): List of all relationship types defined in the schema\\n    relationship_patterns (List[RelationshipPattern]): List of valid relationship patterns', 'properties': {'nodes': {'items': {'$ref': '#/$defs/Node'}, 'title': 'Nodes', 'type': 'array'}, 'relationships': {'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}, 'relationship_patterns': {'items': {'$ref': '#/$defs/RelationshipPattern'}, 'title': 'Relationship Patterns', 'type': 'array'}}, 'required': ['nodes', 'relationships', 'relationship_patterns'], 'title': 'GraphSchema', 'type': 'object'}}}\n",
      "  - run_opencypher_query: {'inputSchema': {'json': {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'parameters': {'anyOf': [{'additionalProperties': True, 'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Parameters'}}, 'required': ['query'], 'title': 'run_opencypher_queryArguments', 'type': 'object'}}, 'name': 'run_opencypher_query', 'description': 'Executes the provided openCypher against the graph.'}\n",
      "  - run_gremlin_query: {'inputSchema': {'json': {'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'run_gremlin_queryArguments', 'type': 'object'}}, 'name': 'run_gremlin_query', 'description': 'Executes the provided Tinkerpop Gremlin against the graph.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Neptune MCP server directly\n",
    "print(\"Testing Neptune MCP Server...\\n\")\n",
    "\n",
    "with create_neptune_mcp_client() as neptune_mcp:\n",
    "    # List available tools\n",
    "    tools = neptune_mcp.list_tools_sync()\n",
    "    print(f\"Available Neptune MCP Tools: {len(tools)}\")\n",
    "\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.tool_name}: {tool.tool_spec}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Strands Agent with Neptune MCP Tools\n",
    "\n",
    "Now let's create an intelligent agent that can interact with Neptune autonomously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent system prompt defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NEPTUNE_AGENT_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert Neptune graph database analyst and assistant.\n",
    "\n",
    "Your capabilities include:\n",
    "1. Checking the status and availability of Neptune graphs\n",
    "2. Analyzing graph schemas to understand data structure\n",
    "3. Generating and executing openCypher queries to retrieve information\n",
    "4. Providing insights and analysis about graph data\n",
    "\n",
    "When analyzing graphs:\n",
    "- Always start by checking the graph status and schema\n",
    "- Generate efficient openCypher queries based on the schema\n",
    "- Explain your findings clearly with relevant statistics\n",
    "- Provide actionable insights from the data\n",
    "\n",
    "When writing openCypher queries:\n",
    "- Use proper syntax for Neptune Analytics/Database\n",
    "- Include LIMIT clauses to avoid overwhelming results\n",
    "- Handle potential errors gracefully\n",
    "- Explain what each query does before executing it\n",
    "\"\"\"\n",
    "\n",
    "print(\"Agent system prompt defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neptune agent creator ready\n"
     ]
    }
   ],
   "source": [
    "# Create the Neptune Graph Analysis Agent\n",
    "def create_neptune_agent():\n",
    "    \"\"\"Create a Strands Agent with Neptune MCP tools.\"\"\"\n",
    "    neptune_mcp = create_neptune_mcp_client()\n",
    "    \n",
    "    with neptune_mcp:\n",
    "        # Get available Neptune tools\n",
    "        tools = neptune_mcp.list_tools_sync()\n",
    "        \n",
    "        # Create the agent\n",
    "        agent = Agent(\n",
    "            system_prompt=NEPTUNE_AGENT_SYSTEM_PROMPT,\n",
    "            tools=tools,\n",
    "        )\n",
    "        \n",
    "        return agent, neptune_mcp\n",
    "\n",
    "print(\"Neptune agent creator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent Demonstrations\n",
    "\n",
    "Let's see the agent in action with various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Basic Graph Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 1: Graph Health Check\n",
      "================================================================================\n",
      "\n",
      "User Prompt: Check the health and status of the Neptune graph. \n",
      "    Provide a summary of its availability and readiness.\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "I'll check the health and status of the Neptune graph for you.\n",
      "Tool #1: get_graph_status\n",
      "## Neptune Graph Status Summary\n",
      "\n",
      "‚úÖ **Graph Status: Available**\n",
      "\n",
      "The Neptune graph is currently **healthy and ready for use**. Here's what this means:\n",
      "\n",
      "- **Availability**: The graph database is online and accessible\n",
      "- **Readiness**: Ready to accept and process queries\n",
      "- **Operations**: You can safely perform read and write operations\n",
      "- **Performance**: The system is operating normally\n",
      "\n",
      "The \"Available\" status indicates that:\n",
      "- All database services are running properly\n",
      "- The graph is ready to handle openCypher and Gremlin queries\n",
      "- No maintenance or backup operations are currently blocking access\n",
      "- The system has sufficient resources to process requests\n",
      "\n",
      "You can now proceed with any graph analysis, querying, or data operations you need to perform. Would you like me to examine the graph schema or run any specific queries to further analyze the data?## Neptune Graph Status Summary\n",
      "\n",
      "‚úÖ **Graph Status: Available**\n",
      "\n",
      "The Neptune graph is currently **healthy and ready for use**. Here's what this means:\n",
      "\n",
      "- **Availability**: The graph database is online and accessible\n",
      "- **Readiness**: Ready to accept and process queries\n",
      "- **Operations**: You can safely perform read and write operations\n",
      "- **Performance**: The system is operating normally\n",
      "\n",
      "The \"Available\" status indicates that:\n",
      "- All database services are running properly\n",
      "- The graph is ready to handle openCypher and Gremlin queries\n",
      "- No maintenance or backup operations are currently blocking access\n",
      "- The system has sufficient resources to process requests\n",
      "\n",
      "You can now proceed with any graph analysis, querying, or data operations you need to perform. Would you like me to examine the graph schema or run any specific queries to further analyze the data?\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TASK 1: Graph Health Check\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "agent, neptune_mcp = create_neptune_agent()\n",
    "\n",
    "with neptune_mcp:\n",
    "    prompt = \"\"\"\n",
    "    Check the health and status of the Neptune graph. \n",
    "    Provide a summary of its availability and readiness.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nUser Prompt: {prompt.strip()}\\n\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = agent(prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Schema Analysis and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 2: Schema Analysis\n",
      "================================================================================\n",
      "\n",
      "User Prompt: Analyze the Neptune graph schema and provide:\n",
      "    1. A summary of the node types (labels) present\n",
      "    2. A summary of the relationship types present\n",
      "    3. Key properties for important node and relationship types\n",
      "    4. An overview of the graph structure and domain it represents\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "I'll help you analyze the Neptune graph schema. Let me start by checking the graph status and then examining the schema structure.\n",
      "Tool #1: get_graph_status\n",
      "Great! The graph is available. Now let me examine the schema to understand the structure of the graph.\n",
      "Tool #2: get_graph_schema\n",
      "Now let me query some sample data to better understand the graph structure and get more detailed information about the properties and data patterns.\n",
      "Tool #3: run_opencypher_query\n",
      "\n",
      "Tool #4: run_opencypher_query\n",
      "Let me examine some sample nodes to better understand the properties and data structure:\n",
      "Tool #5: run_opencypher_query\n",
      "\n",
      "Tool #6: run_opencypher_query\n",
      "\n",
      "Tool #7: run_opencypher_query\n",
      "\n",
      "Tool #8: run_opencypher_query\n",
      "Now let me examine the relationship patterns in more detail:\n",
      "Tool #9: run_opencypher_query\n",
      "\n",
      "Tool #10: run_opencypher_query\n",
      "\n",
      "Tool #11: run_opencypher_query\n",
      "Based on my analysis of the Neptune graph schema and data, here's a comprehensive summary:\n",
      "\n",
      "## 1. Node Types (Labels) Summary\n",
      "\n",
      "The graph contains **3 node types**:\n",
      "\n",
      "- **Entity** (404,302 nodes): The most numerous nodes representing named entities extracted from text\n",
      "- **Chunk** (77,151 nodes): Text segments/chunks from processed documents  \n",
      "- **DocumentId** (17,545 nodes): Unique document identifiers\n",
      "\n",
      "## 2. Relationship Types Summary\n",
      "\n",
      "The graph has **2 relationship types**:\n",
      "\n",
      "- **CONTAINS** (751,280 relationships): Links chunks to the entities they contain/mention\n",
      "- **FROM** (77,151 relationships): Links chunks to their source documents\n",
      "\n",
      "## 3. Key Properties\n",
      "\n",
      "### Chunk Nodes:\n",
      "- `text`: The actual text content of the chunk\n",
      "- `metadata`: General metadata information\n",
      "- `metadata_x-amz-bedrock-kb-source-uri`: S3 URI of the source document\n",
      "- `metadata_x-amz-bedrock-kb-data-source-id`: Bedrock knowledge base data source identifier\n",
      "\n",
      "### Entity Nodes:\n",
      "- No explicit properties stored (entities are identified by their node IDs)\n",
      "- Entity names are embedded in the node IDs (e.g., \"x-amz-bedrock-kb-wakeham\", \"x-amz-bedrock-kb-david thomson\")\n",
      "\n",
      "### DocumentId Nodes:\n",
      "- No explicit properties (identified by encoded document IDs)\n",
      "\n",
      "## 4. Graph Structure and Domain Overview\n",
      "\n",
      "This is a **Knowledge Base Graph** built using **Amazon Bedrock**, specifically designed for:\n",
      "\n",
      "**Domain**: Document processing and entity extraction from a knowledge base\n",
      "**Structure**: A tripartite graph connecting documents ‚Üí chunks ‚Üí entities\n",
      "\n",
      "**Key Patterns**:\n",
      "- **Document Segmentation**: Each document is split into multiple chunks (avg ~4.4 chunks per document)\n",
      "- **Entity Recognition**: Entities are extracted from text chunks and represent people, places, concepts, etc.\n",
      "- **Entity Distribution**: On average, each entity appears in ~1.86 chunks, but some highly mentioned entities appear in up to 1,360 chunks\n",
      "- **Data Source**: Content appears to come from Wikipedia-style articles stored in S3 (\"wikihop-data-files\")\n",
      "\n",
      "**Use Cases**: This graph structure is ideal for:\n",
      "- Semantic search and retrieval\n",
      "- Entity-based question answering\n",
      "- Document similarity analysis\n",
      "- Knowledge graph reasoning\n",
      "- RAG (Retrieval Augmented Generation) applications\n",
      "\n",
      "The graph represents a processed knowledge base where documents have been chunked for better retrieval, and entities have been extracted to enable semantic connections across the corpus.Based on my analysis of the Neptune graph schema and data, here's a comprehensive summary:\n",
      "\n",
      "## 1. Node Types (Labels) Summary\n",
      "\n",
      "The graph contains **3 node types**:\n",
      "\n",
      "- **Entity** (404,302 nodes): The most numerous nodes representing named entities extracted from text\n",
      "- **Chunk** (77,151 nodes): Text segments/chunks from processed documents  \n",
      "- **DocumentId** (17,545 nodes): Unique document identifiers\n",
      "\n",
      "## 2. Relationship Types Summary\n",
      "\n",
      "The graph has **2 relationship types**:\n",
      "\n",
      "- **CONTAINS** (751,280 relationships): Links chunks to the entities they contain/mention\n",
      "- **FROM** (77,151 relationships): Links chunks to their source documents\n",
      "\n",
      "## 3. Key Properties\n",
      "\n",
      "### Chunk Nodes:\n",
      "- `text`: The actual text content of the chunk\n",
      "- `metadata`: General metadata information\n",
      "- `metadata_x-amz-bedrock-kb-source-uri`: S3 URI of the source document\n",
      "- `metadata_x-amz-bedrock-kb-data-source-id`: Bedrock knowledge base data source identifier\n",
      "\n",
      "### Entity Nodes:\n",
      "- No explicit properties stored (entities are identified by their node IDs)\n",
      "- Entity names are embedded in the node IDs (e.g., \"x-amz-bedrock-kb-wakeham\", \"x-amz-bedrock-kb-david thomson\")\n",
      "\n",
      "### DocumentId Nodes:\n",
      "- No explicit properties (identified by encoded document IDs)\n",
      "\n",
      "## 4. Graph Structure and Domain Overview\n",
      "\n",
      "This is a **Knowledge Base Graph** built using **Amazon Bedrock**, specifically designed for:\n",
      "\n",
      "**Domain**: Document processing and entity extraction from a knowledge base\n",
      "**Structure**: A tripartite graph connecting documents ‚Üí chunks ‚Üí entities\n",
      "\n",
      "**Key Patterns**:\n",
      "- **Document Segmentation**: Each document is split into multiple chunks (avg ~4.4 chunks per document)\n",
      "- **Entity Recognition**: Entities are extracted from text chunks and represent people, places, concepts, etc.\n",
      "- **Entity Distribution**: On average, each entity appears in ~1.86 chunks, but some highly mentioned entities appear in up to 1,360 chunks\n",
      "- **Data Source**: Content appears to come from Wikipedia-style articles stored in S3 (\"wikihop-data-files\")\n",
      "\n",
      "**Use Cases**: This graph structure is ideal for:\n",
      "- Semantic search and retrieval\n",
      "- Entity-based question answering\n",
      "- Document similarity analysis\n",
      "- Knowledge graph reasoning\n",
      "- RAG (Retrieval Augmented Generation) applications\n",
      "\n",
      "The graph represents a processed knowledge base where documents have been chunked for better retrieval, and entities have been extracted to enable semantic connections across the corpus.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TASK 2: Schema Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "agent, neptune_mcp = create_neptune_agent()\n",
    "\n",
    "with neptune_mcp:\n",
    "    prompt = \"\"\"\n",
    "    Analyze the Neptune graph schema and provide:\n",
    "    1. A summary of the node types (labels) present\n",
    "    2. A summary of the relationship types present\n",
    "    3. Key properties for important node and relationship types\n",
    "    4. An overview of the graph structure and domain it represents\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nUser Prompt: {prompt.strip()}\\n\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = agent(prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Data Exploration and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 3: Data Exploration and Statistics\n",
      "================================================================================\n",
      "\n",
      "User Prompt: Explore the Neptune graph and provide statistics:\n",
      "    1. Total count of nodes and relationships\n",
      "    2. Count of nodes by type/label\n",
      "    3. Count of relationships by type\n",
      "    4. Any interesting patterns or insights you notice\n",
      "\n",
      "    Please execute appropriate openCypher queries to gather this information.\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "I'll help you explore the Neptune graph and provide comprehensive statistics. Let me start by checking the graph status and schema to understand the data structure.\n",
      "Tool #1: get_graph_status\n",
      "Great! The graph is available. Now let me examine the schema to understand the structure of nodes and relationships.\n",
      "Tool #2: get_graph_schema\n",
      "Perfect! Now I understand the graph structure. This appears to be a knowledge graph with:\n",
      "- **Nodes**: Entity, Chunk, DocumentId\n",
      "- **Relationships**: FROM (Chunk‚ÜíDocumentId), CONTAINS (Chunk‚ÜíEntity)\n",
      "\n",
      "Let me now execute a series of openCypher queries to gather the requested statistics.\n",
      "\n",
      "## 1. Total Count of Nodes and Relationships\n",
      "Tool #3: run_opencypher_query\n",
      "\n",
      "Tool #4: run_opencypher_query\n",
      "## 2. Count of Nodes by Type/Label\n",
      "Tool #5: run_opencypher_query\n",
      "## 3. Count of Relationships by Type\n",
      "Tool #6: run_opencypher_query\n",
      "## 4. Additional Analysis for Patterns and Insights\n",
      "\n",
      "Let me gather some additional statistics to identify interesting patterns:\n",
      "Tool #7: run_opencypher_query\n",
      "Let me fix that query with proper openCypher syntax:\n",
      "Tool #8: run_opencypher_query\n",
      "\n",
      "Tool #9: run_opencypher_query\n",
      "\n",
      "Tool #10: run_opencypher_query\n",
      "\n",
      "Tool #11: run_opencypher_query\n",
      "Let me fix that query:\n",
      "Tool #12: run_opencypher_query\n",
      "## Summary: Neptune Graph Statistics and Insights\n",
      "\n",
      "### üìä **Basic Statistics**\n",
      "\n",
      "1. **Total Nodes**: 498,998\n",
      "2. **Total Relationships**: 828,431\n",
      "\n",
      "### üè∑Ô∏è **Node Distribution by Type**\n",
      "- **Entity**: 404,302 (81.0%) - The most numerous node type\n",
      "- **Chunk**: 77,151 (15.5%) - Text chunks from documents  \n",
      "- **DocumentId**: 17,545 (3.5%) - Source documents\n",
      "\n",
      "### üîó **Relationship Distribution by Type**\n",
      "- **CONTAINS**: 751,280 (90.7%) - Links chunks to entities they contain\n",
      "- **FROM**: 77,151 (9.3%) - Links chunks to their source documents\n",
      "\n",
      "### üîç **Interesting Patterns and Insights**\n",
      "\n",
      "1. **Entity Distribution**: \n",
      "   - Average of **9.74 entities per chunk** (range: 1-70)\n",
      "   - This suggests good entity extraction with reasonable density\n",
      "\n",
      "2. **Document Structure**:\n",
      "   - Average of **4.40 chunks per document** (range: 1-141)\n",
      "   - 1,435 documents have more than 10 chunks, indicating some very large documents\n",
      "\n",
      "3. **Entity Reuse Pattern**:\n",
      "   - **115,066 entities appear in multiple chunks** (28.5% of all entities)\n",
      "   - These shared entities appear in an average of **4.02 chunks each**\n",
      "   - One entity appears in up to **1,360 different chunks** - likely a very common term\n",
      "\n",
      "4. **Graph Connectivity**:\n",
      "   - The graph is well-connected with a **1.66:1 relationship-to-node ratio**\n",
      "   - Perfect consistency: Every chunk has exactly one FROM relationship (77,151 each)\n",
      "\n",
      "### üí° **Key Insights**\n",
      "\n",
      "1. **Knowledge Graph Nature**: This appears to be a knowledge graph built from document processing, likely for RAG (Retrieval-Augmented Generation) applications given the Bedrock metadata properties.\n",
      "\n",
      "2. **Entity Overlap**: The significant number of entities appearing across multiple chunks suggests good cross-referencing and potential for finding related information across documents.\n",
      "\n",
      "3. **Document Variety**: The wide range in document sizes (1-141 chunks) indicates diverse content types in the corpus.\n",
      "\n",
      "4. **Well-Structured Data**: The consistent relationship patterns and clean schema suggest well-processed, structured data suitable for semantic search and analysis.\n",
      "\n",
      "This graph structure is optimal for document-based question answering and semantic search applications, with entities serving as connection points between related content chunks across the document collection.## Summary: Neptune Graph Statistics and Insights\n",
      "\n",
      "### üìä **Basic Statistics**\n",
      "\n",
      "1. **Total Nodes**: 498,998\n",
      "2. **Total Relationships**: 828,431\n",
      "\n",
      "### üè∑Ô∏è **Node Distribution by Type**\n",
      "- **Entity**: 404,302 (81.0%) - The most numerous node type\n",
      "- **Chunk**: 77,151 (15.5%) - Text chunks from documents  \n",
      "- **DocumentId**: 17,545 (3.5%) - Source documents\n",
      "\n",
      "### üîó **Relationship Distribution by Type**\n",
      "- **CONTAINS**: 751,280 (90.7%) - Links chunks to entities they contain\n",
      "- **FROM**: 77,151 (9.3%) - Links chunks to their source documents\n",
      "\n",
      "### üîç **Interesting Patterns and Insights**\n",
      "\n",
      "1. **Entity Distribution**: \n",
      "   - Average of **9.74 entities per chunk** (range: 1-70)\n",
      "   - This suggests good entity extraction with reasonable density\n",
      "\n",
      "2. **Document Structure**:\n",
      "   - Average of **4.40 chunks per document** (range: 1-141)\n",
      "   - 1,435 documents have more than 10 chunks, indicating some very large documents\n",
      "\n",
      "3. **Entity Reuse Pattern**:\n",
      "   - **115,066 entities appear in multiple chunks** (28.5% of all entities)\n",
      "   - These shared entities appear in an average of **4.02 chunks each**\n",
      "   - One entity appears in up to **1,360 different chunks** - likely a very common term\n",
      "\n",
      "4. **Graph Connectivity**:\n",
      "   - The graph is well-connected with a **1.66:1 relationship-to-node ratio**\n",
      "   - Perfect consistency: Every chunk has exactly one FROM relationship (77,151 each)\n",
      "\n",
      "### üí° **Key Insights**\n",
      "\n",
      "1. **Knowledge Graph Nature**: This appears to be a knowledge graph built from document processing, likely for RAG (Retrieval-Augmented Generation) applications given the Bedrock metadata properties.\n",
      "\n",
      "2. **Entity Overlap**: The significant number of entities appearing across multiple chunks suggests good cross-referencing and potential for finding related information across documents.\n",
      "\n",
      "3. **Document Variety**: The wide range in document sizes (1-141 chunks) indicates diverse content types in the corpus.\n",
      "\n",
      "4. **Well-Structured Data**: The consistent relationship patterns and clean schema suggest well-processed, structured data suitable for semantic search and analysis.\n",
      "\n",
      "This graph structure is optimal for document-based question answering and semantic search applications, with entities serving as connection points between related content chunks across the document collection.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TASK 3: Data Exploration and Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "agent, neptune_mcp = create_neptune_agent()\n",
    "\n",
    "with neptune_mcp:\n",
    "    prompt = \"\"\"\n",
    "    Explore the Neptune graph and provide statistics:\n",
    "    1. Total count of nodes and relationships\n",
    "    2. Count of nodes by type/label\n",
    "    3. Count of relationships by type\n",
    "    4. Any interesting patterns or insights you notice\n",
    "    \n",
    "    Please execute appropriate openCypher queries to gather this information.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nUser Prompt: {prompt.strip()}\\n\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = agent(prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Natural Language Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 4: Natural Language Query\n",
      "================================================================================\n",
      "\n",
      "User Prompt: Find the most connected nodes in the graph. \n",
      "    Show me the top 5 nodes with the most relationships,\n",
      "    including their labels and the count of connections.\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "I'll help you find the most connected nodes in the graph. Let me start by checking the graph status and schema to understand the data structure.\n",
      "Tool #1: get_graph_status\n",
      "Great! The graph is available. Now let me examine the schema to understand the structure of nodes and relationships.\n",
      "Tool #2: get_graph_schema\n",
      "Perfect! Now I understand the graph structure. The graph contains:\n",
      "- **Entity** nodes\n",
      "- **Chunk** nodes (with text and metadata properties)\n",
      "- **DocumentId** nodes\n",
      "- **FROM** relationships (Chunk ‚Üí DocumentId)\n",
      "- **CONTAINS** relationships (Chunk ‚Üí Entity)\n",
      "\n",
      "Now I'll write an openCypher query to find the top 5 most connected nodes by counting both incoming and outgoing relationships.\n",
      "Tool #3: run_opencypher_query\n",
      "Excellent! Let me also get some additional context about these highly connected entities to provide more insights.\n",
      "Tool #4: run_opencypher_query\n",
      "Now let me get some statistics about the overall graph structure to provide additional context:\n",
      "Tool #5: run_opencypher_query\n",
      "## Analysis: Most Connected Nodes in the Graph\n",
      "\n",
      "Based on my analysis of your Neptune graph, here are the **top 5 most connected nodes** with their relationship counts:\n",
      "\n",
      "### üèÜ Top 5 Most Connected Nodes\n",
      "\n",
      "1. **\"United States\"** (Entity)\n",
      "   - **1,360 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-united states`\n",
      "\n",
      "2. **\"France\"** (Entity)\n",
      "   - **887 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-france`\n",
      "\n",
      "3. **\"Germany\"** (Entity)\n",
      "   - **637 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-germany`\n",
      "\n",
      "4. **\"London\"** (Entity)\n",
      "   - **568 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-london`\n",
      "\n",
      "5. **\"England\"** (Entity)\n",
      "   - **537 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-england`\n",
      "\n",
      "### üìä Key Insights\n",
      "\n",
      "1. **All top nodes are Entities**: The most connected nodes are all of type \"Entity\", which makes sense given the graph structure where Entities are referenced across multiple text chunks.\n",
      "\n",
      "2. **Geographic pattern**: All top entities represent countries or major cities, suggesting your knowledge base contains substantial content about these geographic locations.\n",
      "\n",
      "3. **Connection pattern**: These entities are connected through `CONTAINS` relationships with Chunk nodes, meaning they are mentioned or referenced in many different text documents.\n",
      "\n",
      "4. **Scale**: With over 404K entities in the graph, these top entities represent the most frequently mentioned concepts across your document corpus.\n",
      "\n",
      "### üîç Graph Structure Context\n",
      "- **Total Entities**: 404,302\n",
      "- **Total Chunks**: 77,151  \n",
      "- **Total Documents**: 17,545\n",
      "- **Relationship types**: FROM (Chunk ‚Üí DocumentId) and CONTAINS (Chunk ‚Üí Entity)\n",
      "\n",
      "The high connectivity of these geographic entities suggests they appear frequently across many different documents in your knowledge base, making them central nodes in the information network.## Analysis: Most Connected Nodes in the Graph\n",
      "\n",
      "Based on my analysis of your Neptune graph, here are the **top 5 most connected nodes** with their relationship counts:\n",
      "\n",
      "### üèÜ Top 5 Most Connected Nodes\n",
      "\n",
      "1. **\"United States\"** (Entity)\n",
      "   - **1,360 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-united states`\n",
      "\n",
      "2. **\"France\"** (Entity)\n",
      "   - **887 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-france`\n",
      "\n",
      "3. **\"Germany\"** (Entity)\n",
      "   - **637 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-germany`\n",
      "\n",
      "4. **\"London\"** (Entity)\n",
      "   - **568 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-london`\n",
      "\n",
      "5. **\"England\"** (Entity)\n",
      "   - **537 connections**\n",
      "   - Node ID: `x-amz-bedrock-kb-england`\n",
      "\n",
      "### üìä Key Insights\n",
      "\n",
      "1. **All top nodes are Entities**: The most connected nodes are all of type \"Entity\", which makes sense given the graph structure where Entities are referenced across multiple text chunks.\n",
      "\n",
      "2. **Geographic pattern**: All top entities represent countries or major cities, suggesting your knowledge base contains substantial content about these geographic locations.\n",
      "\n",
      "3. **Connection pattern**: These entities are connected through `CONTAINS` relationships with Chunk nodes, meaning they are mentioned or referenced in many different text documents.\n",
      "\n",
      "4. **Scale**: With over 404K entities in the graph, these top entities represent the most frequently mentioned concepts across your document corpus.\n",
      "\n",
      "### üîç Graph Structure Context\n",
      "- **Total Entities**: 404,302\n",
      "- **Total Chunks**: 77,151  \n",
      "- **Total Documents**: 17,545\n",
      "- **Relationship types**: FROM (Chunk ‚Üí DocumentId) and CONTAINS (Chunk ‚Üí Entity)\n",
      "\n",
      "The high connectivity of these geographic entities suggests they appear frequently across many different documents in your knowledge base, making them central nodes in the information network.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TASK 4: Natural Language Query\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "agent, neptune_mcp = create_neptune_agent()\n",
    "\n",
    "with neptune_mcp:\n",
    "    prompt = \"\"\"\n",
    "    Find the most connected nodes in the graph. \n",
    "    Show me the top 5 nodes with the most relationships,\n",
    "    including their labels and the count of connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nUser Prompt: {prompt.strip()}\\n\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = agent(prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Complex Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 5: Complex Graph Analysis\n",
      "================================================================================\n",
      "\n",
      "User Prompt: Perform a comprehensive analysis of the graph:\n",
      "    1. Identify the most important node types based on connectivity\n",
      "    2. Find any obvious patterns in the relationship structure\n",
      "    3. Look for potential data quality issues (disconnected nodes, missing properties, etc.)\n",
      "    4. Provide recommendations for graph queries that would be useful for this data\n",
      "\n",
      "    Be thorough and execute multiple queries as needed to gather insights.\n",
      "\n",
      "Agent Response:\n",
      "--------------------------------------------------------------------------------\n",
      "I'll perform a comprehensive analysis of your Neptune graph. Let me start by checking the graph status and understanding its structure.\n",
      "Tool #1: get_graph_status\n",
      "Great! The graph is available. Now let me examine the schema to understand the data structure:\n",
      "Tool #2: get_graph_schema\n",
      "Excellent! Now I have a clear understanding of the graph structure. This appears to be a knowledge graph with document chunks, entities, and documents. Let me perform a comprehensive analysis by executing multiple queries.\n",
      "\n",
      "## 1. Overall Graph Statistics\n",
      "\n",
      "First, let me get basic statistics about node counts and relationship distribution:\n",
      "Tool #3: run_opencypher_query\n",
      "\n",
      "Tool #4: run_opencypher_query\n",
      "## 2. Node Connectivity Analysis\n",
      "\n",
      "Let me analyze the connectivity patterns to identify the most important node types:\n",
      "Tool #5: run_opencypher_query\n",
      "\n",
      "Tool #6: run_opencypher_query\n",
      "\n",
      "Tool #7: run_opencypher_query\n",
      "## 3. Data Quality Analysis\n",
      "\n",
      "Let me check for potential data quality issues:\n",
      "Tool #8: run_opencypher_query\n",
      "\n",
      "Tool #9: run_opencypher_query\n",
      "\n",
      "Tool #10: run_opencypher_query\n",
      "\n",
      "Tool #11: run_opencypher_query\n",
      "\n",
      "Tool #12: run_opencypher_query\n",
      "## 4. Pattern Analysis\n",
      "\n",
      "Let me examine the entity distribution and relationship patterns:\n",
      "Tool #13: run_opencypher_query\n",
      "Let me fix that query:\n",
      "Tool #14: run_opencypher_query\n",
      "\n",
      "Tool #15: run_opencypher_query\n",
      "\n",
      "Tool #16: run_opencypher_query\n",
      "\n",
      "Tool #17: run_opencypher_query\n",
      "## 5. Sample Data Examination\n",
      "\n",
      "Let me examine some sample data to understand the content:\n",
      "Tool #18: run_opencypher_query\n",
      "\n",
      "Tool #19: run_opencypher_query\n",
      "# Comprehensive Graph Analysis Results\n",
      "\n",
      "## 1. Most Important Node Types Based on Connectivity\n",
      "\n",
      "**Entity nodes** are the most critical in this graph with **404,302 nodes** and extremely high connectivity:\n",
      "- **Hub Entities**: Geographic locations dominate the most connected entities:\n",
      "  - \"United States\" (1,360 connections)\n",
      "  - \"France\" (887 connections) \n",
      "  - \"Germany\" (637 connections)\n",
      "  - \"London\" (568 connections)\n",
      "\n",
      "**Key Findings:**\n",
      "- 71.5% of entities (289,236) appear in only 1 chunk (potentially rare/specific concepts)\n",
      "- 25% appear in 2-5 chunks (moderately important concepts)\n",
      "- Only 463 entities (0.1%) are \"super-hubs\" with 50+ connections\n",
      "- Entity connectivity follows a power-law distribution typical of knowledge graphs\n",
      "\n",
      "## 2. Relationship Structure Patterns\n",
      "\n",
      "**Two-Layer Architecture:**\n",
      "```\n",
      "Documents (17,545) ‚Üí Chunks (77,151) ‚Üí Entities (404,302)\n",
      "```\n",
      "\n",
      "**Relationship Distribution:**\n",
      "- **751,280 CONTAINS relationships** (Chunk‚ÜíEntity): Average 9.7 entities per chunk\n",
      "- **77,151 FROM relationships** (Chunk‚ÜíDocument): Each chunk belongs to exactly one document\n",
      "\n",
      "**Document Patterns:**\n",
      "- Average document has 4.4 chunks and 32.5 unique entities\n",
      "- Largest document has 141 chunks and 1,825 entities\n",
      "- This suggests varied document sizes from short articles to comprehensive reports\n",
      "\n",
      "## 3. Data Quality Assessment\n",
      "\n",
      "**Excellent Data Completeness:**\n",
      "‚úÖ **No isolated nodes** - All nodes are properly connected\n",
      "‚úÖ **No missing relationships** - Every chunk has both document and entity connections\n",
      "‚úÖ **100% property coverage** - All chunks have complete metadata\n",
      "‚úÖ **No empty text fields** - All chunks contain actual content\n",
      "\n",
      "**Data Source Distribution:**\n",
      "- Only 2 data sources identified (FIMXQDARXF: 54.4%, T4N5P3XYT1: 45.6%)\n",
      "- Clean, well-structured ingestion from Amazon Bedrock Knowledge Base\n",
      "\n",
      "**Text Quality:**\n",
      "- Consistent chunk sizes (avg: 1,024 characters, median: 1,130)\n",
      "- Reasonable range (3-1,766 characters) suggests proper text segmentation\n",
      "\n",
      "## 4. Recommended Queries for This Dataset\n",
      "\n",
      "Based on the analysis, here are valuable query patterns for this knowledge graph:\n",
      "\n",
      "### **A. Entity-Centric Queries**\n",
      "```cypher\n",
      "// Find documents mentioning specific geographic entities\n",
      "MATCH (d:DocumentId)<-[:FROM]-(c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WHERE id(e) CONTAINS \"united states\"\n",
      "RETURN d, count(c) as relevance_score\n",
      "ORDER BY relevance_score DESC\n",
      "\n",
      "// Discover related entities (co-occurrence analysis)\n",
      "MATCH (c:Chunk)-[:CONTAINS]->(e1:Entity), (c)-[:CONTAINS]->(e2:Entity)\n",
      "WHERE id(e1) = \"x-amz-bedrock-kb-united states\" AND e1 <> e2\n",
      "RETURN id(e2) as related_entity, count(c) as co_occurrence_count\n",
      "ORDER BY co_occurrence_count DESC\n",
      "```\n",
      "\n",
      "### **B. Document Analysis Queries**\n",
      "```cypher\n",
      "// Find the most entity-rich documents\n",
      "MATCH (d:DocumentId)<-[:FROM]-(c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WITH d, count(DISTINCT e) as entity_diversity, count(c) as chunk_count\n",
      "RETURN d, entity_diversity, chunk_count, (entity_diversity * 1.0 / chunk_count) as entity_density\n",
      "ORDER BY entity_diversity DESC\n",
      "\n",
      "// Text search within specific document contexts\n",
      "MATCH (c:Chunk)-[:FROM]->(d:DocumentId)\n",
      "WHERE c.text CONTAINS \"your_search_term\"\n",
      "RETURN d, c.text, c.`metadata_x-amz-bedrock-kb-source-uri`\n",
      "```\n",
      "\n",
      "### **C. Knowledge Discovery Queries**\n",
      "```cypher\n",
      "// Find potential knowledge gaps (entities mentioned only once)\n",
      "MATCH (c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WITH e, count(c) as frequency\n",
      "WHERE frequency = 1\n",
      "RETURN count(e) as rare_concepts\n",
      "\n",
      "// Identify key concepts by data source\n",
      "MATCH (c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WITH c.`metadata_x-amz-bedrock-kb-data-source-id` as source, e, count(c) as frequency\n",
      "WHERE frequency >= 10\n",
      "RETURN source, id(e) as key_entity, frequency\n",
      "ORDER BY source, frequency DESC\n",
      "```\n",
      "\n",
      "### **D. Graph Traversal Queries**\n",
      "```cypher\n",
      "// Multi-hop entity relationships via shared documents\n",
      "MATCH (e1:Entity)<-[:CONTAINS]-(c1:Chunk)-[:FROM]->(d:DocumentId)\n",
      "       <-[:FROM]-(c2:Chunk)-[:CONTAINS]->(e2:Entity)\n",
      "WHERE e1 <> e2\n",
      "RETURN id(e1), id(e2), count(DISTINCT d) as shared_documents\n",
      "ORDER BY shared_documents DESC\n",
      "```\n",
      "\n",
      "## Key Recommendations\n",
      "\n",
      "1. **Leverage Geographic Hub Entities** for geographic analysis and regional content discovery\n",
      "2. **Use entity co-occurrence patterns** for recommendation systems and content similarity\n",
      "3. **Implement entity frequency analysis** for importance scoring and search ranking\n",
      "4. **Consider adding entity types/categories** to enhance query capabilities\n",
      "5. **Monitor the empty entity issue** (443 connections to empty string entity)\n",
      "\n",
      "This is a well-structured knowledge graph ideal for content discovery, entity relationship analysis, and semantic search applications.# Comprehensive Graph Analysis Results\n",
      "\n",
      "## 1. Most Important Node Types Based on Connectivity\n",
      "\n",
      "**Entity nodes** are the most critical in this graph with **404,302 nodes** and extremely high connectivity:\n",
      "- **Hub Entities**: Geographic locations dominate the most connected entities:\n",
      "  - \"United States\" (1,360 connections)\n",
      "  - \"France\" (887 connections) \n",
      "  - \"Germany\" (637 connections)\n",
      "  - \"London\" (568 connections)\n",
      "\n",
      "**Key Findings:**\n",
      "- 71.5% of entities (289,236) appear in only 1 chunk (potentially rare/specific concepts)\n",
      "- 25% appear in 2-5 chunks (moderately important concepts)\n",
      "- Only 463 entities (0.1%) are \"super-hubs\" with 50+ connections\n",
      "- Entity connectivity follows a power-law distribution typical of knowledge graphs\n",
      "\n",
      "## 2. Relationship Structure Patterns\n",
      "\n",
      "**Two-Layer Architecture:**\n",
      "```\n",
      "Documents (17,545) ‚Üí Chunks (77,151) ‚Üí Entities (404,302)\n",
      "```\n",
      "\n",
      "**Relationship Distribution:**\n",
      "- **751,280 CONTAINS relationships** (Chunk‚ÜíEntity): Average 9.7 entities per chunk\n",
      "- **77,151 FROM relationships** (Chunk‚ÜíDocument): Each chunk belongs to exactly one document\n",
      "\n",
      "**Document Patterns:**\n",
      "- Average document has 4.4 chunks and 32.5 unique entities\n",
      "- Largest document has 141 chunks and 1,825 entities\n",
      "- This suggests varied document sizes from short articles to comprehensive reports\n",
      "\n",
      "## 3. Data Quality Assessment\n",
      "\n",
      "**Excellent Data Completeness:**\n",
      "‚úÖ **No isolated nodes** - All nodes are properly connected\n",
      "‚úÖ **No missing relationships** - Every chunk has both document and entity connections\n",
      "‚úÖ **100% property coverage** - All chunks have complete metadata\n",
      "‚úÖ **No empty text fields** - All chunks contain actual content\n",
      "\n",
      "**Data Source Distribution:**\n",
      "- Only 2 data sources identified (FIMXQDARXF: 54.4%, T4N5P3XYT1: 45.6%)\n",
      "- Clean, well-structured ingestion from Amazon Bedrock Knowledge Base\n",
      "\n",
      "**Text Quality:**\n",
      "- Consistent chunk sizes (avg: 1,024 characters, median: 1,130)\n",
      "- Reasonable range (3-1,766 characters) suggests proper text segmentation\n",
      "\n",
      "## 4. Recommended Queries for This Dataset\n",
      "\n",
      "Based on the analysis, here are valuable query patterns for this knowledge graph:\n",
      "\n",
      "### **A. Entity-Centric Queries**\n",
      "```cypher\n",
      "// Find documents mentioning specific geographic entities\n",
      "MATCH (d:DocumentId)<-[:FROM]-(c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WHERE id(e) CONTAINS \"united states\"\n",
      "RETURN d, count(c) as relevance_score\n",
      "ORDER BY relevance_score DESC\n",
      "\n",
      "// Discover related entities (co-occurrence analysis)\n",
      "MATCH (c:Chunk)-[:CONTAINS]->(e1:Entity), (c)-[:CONTAINS]->(e2:Entity)\n",
      "WHERE id(e1) = \"x-amz-bedrock-kb-united states\" AND e1 <> e2\n",
      "RETURN id(e2) as related_entity, count(c) as co_occurrence_count\n",
      "ORDER BY co_occurrence_count DESC\n",
      "```\n",
      "\n",
      "### **B. Document Analysis Queries**\n",
      "```cypher\n",
      "// Find the most entity-rich documents\n",
      "MATCH (d:DocumentId)<-[:FROM]-(c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WITH d, count(DISTINCT e) as entity_diversity, count(c) as chunk_count\n",
      "RETURN d, entity_diversity, chunk_count, (entity_diversity * 1.0 / chunk_count) as entity_density\n",
      "ORDER BY entity_diversity DESC\n",
      "\n",
      "// Text search within specific document contexts\n",
      "MATCH (c:Chunk)-[:FROM]->(d:DocumentId)\n",
      "WHERE c.text CONTAINS \"your_search_term\"\n",
      "RETURN d, c.text, c.`metadata_x-amz-bedrock-kb-source-uri`\n",
      "```\n",
      "\n",
      "### **C. Knowledge Discovery Queries**\n",
      "```cypher\n",
      "// Find potential knowledge gaps (entities mentioned only once)\n",
      "MATCH (c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WITH e, count(c) as frequency\n",
      "WHERE frequency = 1\n",
      "RETURN count(e) as rare_concepts\n",
      "\n",
      "// Identify key concepts by data source\n",
      "MATCH (c:Chunk)-[:CONTAINS]->(e:Entity)\n",
      "WITH c.`metadata_x-amz-bedrock-kb-data-source-id` as source, e, count(c) as frequency\n",
      "WHERE frequency >= 10\n",
      "RETURN source, id(e) as key_entity, frequency\n",
      "ORDER BY source, frequency DESC\n",
      "```\n",
      "\n",
      "### **D. Graph Traversal Queries**\n",
      "```cypher\n",
      "// Multi-hop entity relationships via shared documents\n",
      "MATCH (e1:Entity)<-[:CONTAINS]-(c1:Chunk)-[:FROM]->(d:DocumentId)\n",
      "       <-[:FROM]-(c2:Chunk)-[:CONTAINS]->(e2:Entity)\n",
      "WHERE e1 <> e2\n",
      "RETURN id(e1), id(e2), count(DISTINCT d) as shared_documents\n",
      "ORDER BY shared_documents DESC\n",
      "```\n",
      "\n",
      "## Key Recommendations\n",
      "\n",
      "1. **Leverage Geographic Hub Entities** for geographic analysis and regional content discovery\n",
      "2. **Use entity co-occurrence patterns** for recommendation systems and content similarity\n",
      "3. **Implement entity frequency analysis** for importance scoring and search ranking\n",
      "4. **Consider adding entity types/categories** to enhance query capabilities\n",
      "5. **Monitor the empty entity issue** (443 connections to empty string entity)\n",
      "\n",
      "This is a well-structured knowledge graph ideal for content discovery, entity relationship analysis, and semantic search applications.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Complex analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 5: Complex Graph Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "agent, neptune_mcp = create_neptune_agent()\n",
    "\n",
    "with neptune_mcp:\n",
    "    # This prompt should be customized based on your graph domain\n",
    "    prompt = \"\"\"\n",
    "    Perform a comprehensive analysis of the graph:\n",
    "    1. Identify the most important node types based on connectivity\n",
    "    2. Find any obvious patterns in the relationship structure\n",
    "    3. Look for potential data quality issues (disconnected nodes, missing properties, etc.)\n",
    "    4. Provide recommendations for graph queries that would be useful for this data\n",
    "    \n",
    "    Be thorough and execute multiple queries as needed to gather insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nUser Prompt: {prompt.strip()}\\n\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = agent(prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

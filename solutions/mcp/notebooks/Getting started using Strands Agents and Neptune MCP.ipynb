{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fe4fae",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: MIT-0\n",
    "\n",
    "# Getting Started Guide to using Strands Agents and Neptune MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ed32a",
   "metadata": {},
   "source": [
    "In this notebook, we're going to demonstrate how to use Amazon GenAI tools alongside Amazon's graph database, Amazon Neptune, in order to build a knowledge graph containing data relating to Amazon Neptune, and entities extracting directly from a conversation.\n",
    "\n",
    "In this demo we will be using the following tools and services:\n",
    "\n",
    "- [Amazon Bedrock](https://aws.amazon.com/bedrock)\n",
    "- [Amazon Neptune Analytics](https://aws.amazon.com/neptune)\n",
    "- [Strands Agent SDK](https://strandsagents.com/latest/)\n",
    "- [Neptune MCP Server](https://github.com/awslabs/mcp/tree/main/src/amazon-neptune-mcp-server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60127260",
   "metadata": {},
   "source": [
    "## Setting up the Neptune MCP Server\n",
    "In order to use the [Neptune MCP server](https://github.com/awslabs/mcp/tree/main/src/amazon-neptune-mcp-server), we must first download it from the AWS Labs GitHub repository and build it using Docker. We can do this using the following process. \n",
    "\n",
    "Navigate to the [folder view](../../tree) open a new terminal window and execute each of the following commands in turn:\n",
    "\n",
    "```\n",
    "    cd SageMaker/\n",
    "    git clone https://github.com/awslabs/mcp.git\n",
    "    cd mcp/\n",
    "    cd src/\n",
    "    cd amazon-neptune-mcp-server/\n",
    "    docker build -t awslabs/amazon-neptune-mcp-server .\n",
    "    docker images\n",
    "    docker run awslabs/amazon-neptune-mcp-server .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc21d77",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Now that we have the Neptune MCP Server running, we need to install the correct Python packages to run it, and to use the [Strands SDK](https://strandsagents.com/0.1.x/user-guide/concepts/tools/mcp-tools/) library. Running the following command will install these packages locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install strands-agents strands-agents-tools uv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv python install 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95f1f8",
   "metadata": {},
   "source": [
    "To extract and format information from the Neptune public documentation, we also need to install the Beautiful Soup library. We'll be using this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b82b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "MEMORY_PROMPT = \"\"\"\n",
    "                For all identified entities and connections, save them to the Amazon Neptune graph.\n",
    "                Do not create duplicate entities. If you identify an entity already exists, use that instead.\n",
    "            \"\"\"\n",
    "\n",
    "QUERY_PROMPT = \"\"\"\n",
    "                You are an agent that interacts with an Amazon Neptune database to run graph queries. \n",
    "                Whenever you write queries you should first fetch the schema to ensure that you understand \n",
    "                the correct labels and property names as well as the appropriate casing of those names and values.\n",
    "            \"\"\"\n",
    "\n",
    "USE_NEPTUNE_ANALYTICS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAPH_ID is the ID of your Neptune Analytics graph, and will be in the format \"g-abc123\". This is not used if you're working with Neptune Database.\n",
    "GRAPH_ID = \"<UPDATE WITH THE NEPTUNE GRAPH ID>\"\n",
    "#GRAPH_ENDPOINT is the primary endpoint of your Neptune Analytics graph or Neptune Database cluster\n",
    "GRAPH_ENDPOINT = \"<UPDATE WITH THE NEPTUNE CLUSTER/GRAPH ENDPOINT>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d97fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_endpoint(is_neptune_analytics):\n",
    "    #check if we're connecting to Neptune Analytics or Neptune Database\n",
    "    graph_endpoint = \"\"\n",
    "    if USE_NEPTUNE_ANALYTICS:\n",
    "        graph_endpoint = f\"neptune-graph://{GRAPH_ID}\" #for Neptune Analytics\n",
    "    else:\n",
    "        graph_endpoint = f\"neptune-db://{GRAPH_ENDPOINT}\" #for Neptune Database\n",
    "\n",
    "    print(f\"Using '{graph_endpoint}' as the graph endpoint.\")\n",
    "    return graph_endpoint\n",
    "\n",
    "# creates and returns a new instance of the MCPClient\n",
    "def create_mcp_client():\n",
    "    endpoint = get_graph_endpoint(USE_NEPTUNE_ANALYTICS)\n",
    "\n",
    "    return MCPClient(lambda: stdio_client(StdioServerParameters(\n",
    "        command=\"uvx\", \n",
    "        args=[\"awslabs.amazon-neptune-mcp-server@latest\"],\n",
    "        env={\"NEPTUNE_ENDPOINT\": endpoint}\n",
    "    )))\n",
    "\n",
    "# executes a read query on the graph\n",
    "def run_agent_read_query(question, return_response=False):\n",
    "    memory_mcp_client = create_mcp_client()\n",
    "\n",
    "    with memory_mcp_client:\n",
    "        tools = memory_mcp_client.list_tools_sync()\n",
    "        agent = Agent(tools=tools, \n",
    "                      model=BedrockModel(model_id=model_id),\n",
    "                      system_prompt=QUERY_PROMPT\n",
    "                )\n",
    "        r = agent(question)\n",
    "        if return_response:\n",
    "            return r\n",
    "\n",
    "def run_agent_clean_up_query():\n",
    "    memory_mcp_client = create_mcp_client()\n",
    "    \n",
    "    with memory_mcp_client:\n",
    "        tools = memory_mcp_client.list_tools_sync()\n",
    "        agent = Agent(tools=tools, \n",
    "                      model=BedrockModel(model_id=model_id),\n",
    "                      system_prompt=MEMORY_PROMPT\n",
    "                )\n",
    "        \n",
    "        question = f\"\"\"\n",
    "            Retrieve information about all the nodes in the graph. Where you find duplicate nodes with the same or similar \n",
    "            name, description, label and properties choose one of the duplicates to be the primary node. \n",
    "            Copy all the relationships between all the other matching duplicate nodes and object nodes, and recreate \n",
    "            them connecting to the primary node. \n",
    "            Only do this where the same relationship between the primary and object node doesn't already exist. \n",
    "            Once all the relationships have been recreated where necessary, add the \"Duplicate\" label to all the other \n",
    "            similar duplicates, excluding the selected primary node.\n",
    "        \"\"\"\n",
    "        \n",
    "        agent(question)\n",
    "    \n",
    "# executes a mutation/write query on the graph\n",
    "def run_agent_write_query(query):\n",
    "    memory_mcp_client = create_mcp_client()\n",
    "\n",
    "    with memory_mcp_client:\n",
    "        tools = memory_mcp_client.list_tools_sync()\n",
    "        agent = Agent(tools=tools, \n",
    "                      model=BedrockModel(model_id=model_id),\n",
    "                      system_prompt=MEMORY_PROMPT\n",
    "                )\n",
    "        \n",
    "        question = f\"\"\"\n",
    "            From the following data, create new nodes based on the entities you identify. \n",
    "            Only create new entities if required, or use existing ones if they're already in the graph. \n",
    "            Capture as much detail as possible to create a highly connected graph.\n",
    "            \n",
    "            ##DATA##\n",
    "            {query}\n",
    "        \"\"\"\n",
    "        \n",
    "        agent(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8087b55",
   "metadata": {},
   "source": [
    "## Integrating Neptune with GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2fab2",
   "metadata": {},
   "source": [
    "### Loading data from  files\n",
    "\n",
    "The following code will read from several web pages in the Neptune public documentation, and send the contents to the Neptune MCP server to be stored into our graph. We'll initialise the graph with the contents to build up some information about the service in our knowledge graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.aws.amazon.com/neptune/latest/userguide/intro.html\",\n",
    "    \"https://docs.aws.amazon.com/neptune/latest/userguide/graph-get-started.html\",\n",
    "    \"https://docs.aws.amazon.com/neptune/latest/userguide/neptune-setup.html\",\n",
    "    \"https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html#neptune-analytics-vs-neptune-database\"\n",
    "]\n",
    "\n",
    "for u in urls:\n",
    "    c = requests.get(u)\n",
    "    html = c.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    message = f\"\"\"\n",
    "        Here is information about the Amazon Neptune service.\n",
    "        {soup.body}\n",
    "    \"\"\"\n",
    "\n",
    "    run_agent_write_query(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c7c71",
   "metadata": {},
   "source": [
    "#### Cleaning up the graph model\n",
    "\n",
    "Due to the way LLMs process the data, the graph can become populated with entities that represent the same real-life thing. For example, `Amazon Neptune`, `Amazon Neptune Database` and `Neptune Database` all represent the same `Amazon Neptune Database` service entity. \n",
    "\n",
    "As a result, we can run a further query to direct the agent to identify these duplicate entities, re-route the connected relationships, and then either perform a hard-delete, e.g. remove the duplicates from the graph, or a soft-delete, by applying a marker to the duplicate entries. In this example, we're performing a soft-delete by applying a `Duplicate` label using the following prompt:\n",
    "\n",
    "```\n",
    "    Retrieve information about all the nodes in the graph. Where you find duplicate nodes with the same or similar \n",
    "    name, description, label and properties choose one of the duplicates to be the primary node. \n",
    "    Copy all the relationships between all the other matching duplicate nodes and object nodes, and recreate \n",
    "    them connecting to the primary node. \n",
    "    Only do this where the same relationship between the primary and object node doesn't already exist. \n",
    "    Once all the relationships have been recreated where necessary, add the \"Duplicate\" label to all the other \n",
    "    similar duplicates, excluding the selected primary node.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent_clean_up_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020be06",
   "metadata": {},
   "source": [
    "### Exploring our graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee3be2",
   "metadata": {},
   "source": [
    "As we'll be running graph queries over our data set, let's now ensure the notebook is pointing to the correct graph. Using the `%graph_notebook_host` [line magic](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebooks-line-magics-graph-notebook-host) command provides us with the functionality to set the current notebook to a specific graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%graph_notebook_host {get_graph_endpoint(USE_NEPTUNE_ANALYTICS)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35955a0",
   "metadata": {},
   "source": [
    "Let's now take a look at what's been saved to our knowledge graph. We can do this using openCypher, an open-standard query language that both Neptune Database and Neptune Analytics support. The Strands agent and LLM creates nodes with a label based on their identified type, e.g. Service, ServiceComponent, QueryLanguage, Feature, etc. It also creates properties based on the information extracted by the LLM for each entity.\n",
    "\n",
    "An example of this is shown below:\n",
    "\n",
    "- `~labels`: ServiceFeature\n",
    "- `name`: Cluster Volume\n",
    "- `description`: Neptune data is stored in the cluster volume, designed for reliability and high availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096fc1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%oc\n",
    "MATCH p = ()-[*1..3]->()\n",
    "RETURN p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce0b0a",
   "metadata": {},
   "source": [
    "### Asking natural language-based questions\n",
    "\n",
    "It's often the case that the job functions who will be interacting with the graph will not know how to write openCypher, or know how the graph is connected. In this case, we can utilise Strands and the Neptune MCP server to perform read queries for a given natural language question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8245ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent_read_query(\n",
    "    \"\"\"\n",
    "        What are the differences between Neptune Database and Neptune Analytics?\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8a472",
   "metadata": {},
   "source": [
    "The above query produces a list of performed actions as well as the final response, which contains a summary of all the source nodes and relationships found in the graph. It should look similar to the following:\n",
    "\n",
    "```\n",
    "To answer your question about the differences between Neptune Database and Neptune Analytics, I need to understand what kind of information we have available in the Neptune graph. Let me first check the graph status and schema to determine what information we can retrieve.\n",
    "Tool #1: get_graph_status\n",
    "\n",
    "Tool #2: get_graph_schema\n",
    "Now, let me look for information about Neptune Database and Neptune Analytics in the graph:\n",
    "Tool #3: run_opencypher_query\n",
    "Let's get more detailed information about the features of each service:\n",
    "\n",
    "... // removed for brevity\n",
    "\n",
    "Based on the database information, I can provide a comparison between Amazon Neptune Database and Amazon Neptune Analytics:\n",
    "\n",
    "# Differences Between Neptune Database and Neptune Analytics\n",
    "\n",
    "## Amazon Neptune Database\n",
    "- **Primary Purpose**: A managed graph database service for storing and querying highly connected data\n",
    "- **Key Features**:\n",
    "  - Fully managed graph database service\n",
    "  - High availability (>99.99% availability)\n",
    "  - Security features including VPC isolation and encryption at rest\n",
    "  - Various instance and storage types available\n",
    "  - Data loading capabilities including bulk loading and streaming\n",
    "  - Provides connectivity, cluster creation, and monitoring tools\n",
    "\n",
    "## Amazon Neptune Analytics\n",
    "- **Primary Purpose**: A memory-optimized graph database engine specifically designed for analytics\n",
    "- **Key Features**:\n",
    "  - Memory optimization for storing large graph datasets in memory\n",
    "  - Graph analytic algorithms library\n",
    "  - Low-latency graph queries for fast data processing\n",
    "  - Vector search capabilities within graph traversals\n",
    "  - Designed to quickly analyze large amounts of graph data in seconds\n",
    "\n",
    "## Relationship\n",
    "- The two services **complement** each other, with Neptune Analytics enhancing the capabilities of Neptune Database\n",
    "- Neptune Database focuses on operational graph database needs (storage, querying, management)\n",
    "- Neptune Analytics focuses on analytical processing and deriving insights from large graph datasets\n",
    "\n",
    "## Use Case Differences\n",
    "- **Neptune Database**: Best for transactional workloads, real-time querying, and storing graph data\n",
    "- **Neptune Analytics**: Best for analytical workloads, pattern detection, finding trends, and complex graph algorithms over large datasets\n",
    "\n",
    "In summary, Neptune Database is designed for storing and managing graph data with ACID compliance in a transactional environment, while Neptune Analytics is optimized for performing fast analytical operations on large graph datasets. They work together as complementary services, with analytics extending the capabilities of the base database service.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551d722",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The combination of Neptune, Bedrock, Strands Agents and MCP provides a consistently powerful mechanism for ingesting data into and reading data from a knowledge graph to identify common patterns, trends and information that would otherwise be difficult to navigate or locate using traditional database and RAG methods.\n",
    "\n",
    "By using a simple functions to read the contents from a web page, conversation or file, we can send the contents to an agent to perform entity identification, extraction and relationship generation. This process greatly simplifies production of highly connected knowledge graphs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

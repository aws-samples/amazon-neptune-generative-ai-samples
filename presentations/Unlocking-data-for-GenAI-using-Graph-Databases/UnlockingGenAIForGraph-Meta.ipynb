{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b136892",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In this demo, we are playing a [phrasal template](https://en.wikipedia.org/wiki/Phrasal_template) game where we will substitute different phrases and words into realistic financial news stories and perform RAG and GraphRAG queries on them. The goal is to use fictional names and facts to demonstrate 1/ the LLM is not answering based on prior factual training, and 2/ this is not a pre-trained or canned scenario optimized for the specific stories.  To get things started, I've populated all of the fields, but you are free to change whatever you like to another value within the theme suggested!\n",
    "\n",
    "This demonstration is optimized for the Meta Llama 3.2 family of models.  You will get the best results using the [Meta Llama 3.2 90B model](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-llama-3.2-vision-models-(11b/90b)-) for entity extraction and question answering, but you can choose which model you want to choose!\n",
    "\n",
    "This notebook is used as a demonstration to accompany [this presentation](https://github.com/aws-samples/amazon-neptune-generative-ai-samples/presentations/Unlocking-data-for-GenAI-using-Graph-Databases/Unlocking_data_for_GenAI_using_Graph_Databases_Presentation.pdf)\n",
    "\n",
    "### Prerequisites\n",
    "This demonstration requires a Neptune Analytics graph (32 m-NCU is fine), a Neptune Notebook instance, access to the [Amazon Titan Text Embeddings v2 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html), access to a [Meta Llama 3.2 model in Bedrock](https://aws.amazon.com/bedrock/llama/), and an Amazon Simple Storage Service (S3) bucket to stage your graph content before loading into Neptune.  \n",
    "\n",
    "To create a new graph, follow the instructions in the [Neptune Analytics documentation](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/create-graph-using-console.html).  You do not need to have a replica, so choose 0 instead of the default of 1. \n",
    "\n",
    "You must have access to the Titan and Meta Llama models in Bedrock. Follow the instructions in the [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) before executing the steps in this notebook.\n",
    "\n",
    "You must have a Neptune Notebook instance. Follow the instructions in the [Neptune User Guide](https://docs.aws.amazon.com/neptune/latest/userguide/graph-notebooks.html) to create a Neptune Notebook. An ml.t3.medium instance size is fine for this demo.\n",
    "\n",
    "To create an S3 Bucket, follow these instructions in the [Amazon S3 User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html).\n",
    "\n",
    "### Costs\n",
    "The approximate costs for running this are as high as $20 if completed once in an hour or less in US East 1. The costs are broken down as such:<br>\n",
    "Neptune Analytics Graph \\(32 m-NCUs\\)\\: \\\\$0.96/hour<br>\n",
    "Neptune Notebook Instance \\(ml.t3.medium\\)\\: \\\\$0.05/hour<br>\n",
    "S3 Bucket Usage: This will fall under the free tier, or will cost approximately \\\\$0.01 if you do not qualify for free tier.<br>\n",
    "Bedrock+Llama Model Usage\\:\n",
    "- RAG encodings cost: approximately \\\\$0.03 \\(4 documents + question using Amazon Titan Text Embeddings v2\\)\n",
    "- One RAG summarization answer cost approximately between \\\\$0.0775 \\(Llama 3.2 1B\\) to \\\\$1.55 \\(Llama 3.2 \n",
    "90B\\) depending on the model used\n",
    "- The GraphRAG encodings for all 4 documents and the question cost approximately \\\\$0.61 \\(1B\\) to \\\\$12.32 \\(90B\\) depending on the model used\n",
    "- The GraphRAG answer cost approximately between \\\\$0.2454 \\(1B\\) to \\\\$4.908 \\(90B\\) depending on the model used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76beb7e6",
   "metadata": {},
   "source": [
    "### Notebook configuration\n",
    "\n",
    "Substitute your graph identifier below for `$YOUR-GRAPH-NAME$`. You can find the graph identifier in the console or using the AWS API.  It will have a format such as `g-1234abcd5e`.  Substitute your region for `$YOUR-AWS-REGION$` in both the `host` and `aws_region` variables, such as `us-east-1`.  Select the `run` button to save the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb63b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%graph_notebook_config\n",
    "\n",
    "{\n",
    "  \"host\": \"$YOUR-GRAPH-NAME$.$YOUR-AWS-REGION$.neptune-graph.amazonaws.com\",\n",
    "  \"neptune_service\": \"neptune-graph\",\n",
    "  \"port\": 8182,\n",
    "  \"proxy_host\": \"\",\n",
    "  \"proxy_port\": 8182,\n",
    "  \"auth_mode\": \"IAM\",\n",
    "  \"load_from_s3_arn\": \"\",\n",
    "  \"ssl\": true,\n",
    "  \"ssl_verify\": true,\n",
    "  \"aws_region\": \"$YOUR-AWS-REGION$\",\n",
    "  \"sparql\": {\n",
    "    \"path\": \"sparql\"\n",
    "  },\n",
    "  \"gremlin\": {\n",
    "    \"traversal_source\": \"g\",\n",
    "    \"username\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"message_serializer\": \"GraphSONUntypedMessageSerializerV4\"\n",
    "  },\n",
    "  \"neo4j\": {\n",
    "    \"username\": \"neo4j\",\n",
    "    \"password\": \"password\",\n",
    "    \"auth\": true,\n",
    "    \"database\": null\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2f32c",
   "metadata": {},
   "source": [
    "Run the next two cells to change the display settings to properly format overlapping edges, and load the graph configuration into a variable so we can use it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%graph_notebook_vis_options\n",
    "{\n",
    "  \"nodes\": {\n",
    "    \"borderWidthSelected\": 0,\n",
    "    \"borderWidth\": 0,\n",
    "    \"color\": {\n",
    "      \"background\": \"rgba(210, 229, 255, 1)\",\n",
    "      \"border\": \"transparent\",\n",
    "      \"highlight\": {\n",
    "        \"background\": \"rgba(9, 104, 178, 1)\",\n",
    "        \"border\": \"rgba(8, 62, 100, 1)\"\n",
    "      }\n",
    "    },\n",
    "    \"shadow\": {\n",
    "      \"enabled\": false\n",
    "    },\n",
    "    \"shape\": \"circle\",\n",
    "    \"widthConstraint\": {\n",
    "      \"minimum\": 70,\n",
    "      \"maximum\": 70\n",
    "    },\n",
    "    \"font\": {\n",
    "      \"face\": \"courier new\",\n",
    "      \"color\": \"black\",\n",
    "      \"size\": 12\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"color\": {\n",
    "      \"inherit\": false\n",
    "    },\n",
    "    \"smooth\": {\n",
    "      \"enabled\": true,\n",
    "      \"type\": \"dynamic\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"type\": \"arrow\"\n",
    "      }\n",
    "    },\n",
    "    \"font\": {\n",
    "      \"face\": \"courier new\"\n",
    "    }\n",
    "  },\n",
    "  \"interaction\": {\n",
    "    \"hover\": true,\n",
    "    \"hoverConnectedEdges\": true,\n",
    "    \"selectConnectedEdges\": false\n",
    "  },\n",
    "  \"physics\": {\n",
    "    \"minVelocity\": 0.75,\n",
    "    \"barnesHut\": {\n",
    "      \"centralGravity\": 0.1,\n",
    "      \"gravitationalConstant\": -50450,\n",
    "      \"springLength\": 95,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.09,\n",
    "      \"avoidOverlap\": 0.1\n",
    "    },\n",
    "    \"solver\": \"barnesHut\",\n",
    "    \"enabled\": true,\n",
    "    \"adaptiveTimestep\": true,\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 1\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%graph_notebook_config --store-to config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d92a9",
   "metadata": {},
   "source": [
    "### Configure Connections and S3 Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cd8ed",
   "metadata": {},
   "source": [
    "Substitute `$BUCKET-NAME-HERE$` with the name of the S3 bucket name you are using. Add just the bucket name, not the full URI.  Then run this cell and the next cell to set up the connections we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17260c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = \"$BUCKET-NAME-HERE$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "endpoint_url = \"https://\" + json.loads(config)['host'] + \":\" + str(json.loads(config)['port'])\n",
    "region = json.loads(config)['aws_region']\n",
    "neptune_client = boto3.client('neptune-graph')\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "\n",
    "s3_bucket = f\"s3://{s3_bucket_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efc245",
   "metadata": {},
   "source": [
    "### Code and helper functions\n",
    "\n",
    "This section contains a lot of helper function code. Describing this code is outside the scope of this demonstration, but feel free to explore it yourself. Please remember this is demonstration code only.  Run this cell to execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c862be",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "from enum import Enum\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import html\n",
    "from json import JSONEncoder\n",
    "import uuid\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"__json__\", _default.default)(obj)\n",
    "\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "\n",
    "context = dict(\n",
    "    completion_delimiter=\"||COMPLETE||\",\n",
    "    tuple_delimiter=\"|~|\",\n",
    "    record_delimiter=\"##\",\n",
    "    llm_temperature=0.25,\n",
    "    llm_top_p = 0.25\n",
    ")\n",
    "\n",
    "connections_template = \"\"\"\n",
    "MATCH (entity1)-[event*1..]-(entity2)\n",
    "WHERE $entity1_text = entity1.name AND UPPER($entity1_label) IN LABELS(entity1)\n",
    "AND (($entity2_text = entity2.name AND UPPER($entity2_label) IN LABELS(entity2))\n",
    "RETURN STARTNODE(event).name as subject, LABELS(STARTNODE(event))[0] as subjectType, TYPE(event) as eventType, ENDNODE(event).name as target, LABELS(ENDNODE(event))[0] as targetType\n",
    "\"\"\"\n",
    "\n",
    "inquiry_template = \"\"\"\n",
    "MATCH (entity1)-[event]-(entity2)-[event2]-(entity3)\n",
    "WHERE $entity1_text = entity1.name AND UPPER($entity1_label) IN LABELS(entity1)\n",
    "AND entity1 <> entity3 AND event <> event2\n",
    "WITH event\n",
    "WHERE TYPE(event) <> \"hasChunk\"\n",
    "RETURN STARTNODE(event).name as subject, LABELS(STARTNODE(event))[0] as subjectType, TYPE(event) as eventType, ENDNODE(event).name as target, LABELS(ENDNODE(event))[0] as targetType\n",
    "UNION\n",
    "MATCH (entity1)-[event]-(entity2)-[event2]-(entity3)\n",
    "WHERE $entity1_text = entity1.name AND UPPER($entity1_label) IN LABELS(entity1)\n",
    "AND entity1 <> entity3 AND event <> event2\n",
    "WITH event2\n",
    "WHERE TYPE(event2) <> \"hasChunk\"\n",
    "RETURN STARTNODE(event2).name as subject, LABELS(STARTNODE(event2))[0] as subjectType, TYPE(event2) as eventType, ENDNODE(event2).name as target, LABELS(ENDNODE(event2))[0] as targetType\n",
    "\"\"\"\n",
    "\n",
    "chunk_retrieval_template = \"\"\"\n",
    "MATCH (entity1)-[event*0..1]-(entity2)-[event2:hasChunk]->(chunk:Chunk)\n",
    "WHERE $entity1_text = entity1.name AND UPPER($entity1_label) IN LABELS(entity1)\n",
    "RETURN DISTINCT chunk.description as text_chunk\n",
    "\"\"\"\n",
    "\n",
    "##################################################################################################\n",
    "#\n",
    "#  Parts of this code and techniques for use of the LLM were influenced by LightRAG: Simple and Fast Retrieval-Augmented Generation\n",
    "#  Authors: Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang\n",
    "#  URL: https://github.com/HKUDS/LightRAG/\n",
    "#\n",
    "#\n",
    "#\n",
    "##################################################################################################\n",
    "\n",
    "def run_llm(text, context, bedrock_client, model_id, prompt, do_formatting = True, cost_tracking = None):\n",
    "    \n",
    "    request_text = {\n",
    "        \"max_gen_len\": 2048,\n",
    "        \"temperature\": context[\"llm_temperature\"],\n",
    "        \"top_p\": context[\"llm_top_p\"],\n",
    "        \"prompt\": prompt.format(**context, input_text=text) if do_formatting else prompt\n",
    "    }\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(request_text)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = bedrock_client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Decode the response body.\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    if not (cost_tracking is None):\n",
    "        cost_tracking[\"prompt_tokens\"] = cost_tracking[\"prompt_tokens\"] + response_body[\"prompt_token_count\"] if \"prompt_tokens\" in cost_tracking else response_body[\"prompt_token_count\"]\n",
    "        cost_tracking[\"generation_tokens\"] = cost_tracking[\"generation_tokens\"] + response_body[\"generation_token_count\"] if \"generation_tokens\" in cost_tracking else response_body[\"generation_token_count\"]\n",
    "\n",
    "    return response_body[\"generation\"]\n",
    "def run_llm_with_history(text, context, bedrock_client, model_id, prompt, history, do_formatting = True):\n",
    "    \n",
    "    prompt = \"\\n\".join(history)\n",
    "    \n",
    "    request_text = {\n",
    "        \"max_gen_len\": 2048,\n",
    "        \"temperature\": context[\"llm_temperature\"],\n",
    "        \"top_p\": context[\"llm_top_p\"],\n",
    "        \"prompt\": prompt + \"\\n\" + (prompt.format(**context, input_text=text) if do_formatting else prompt)\n",
    "    }\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(request_text)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = bedrock_client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Decode the response body.\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    return response_body[\"generation\"]\n",
    "def run_llm_batch(text_chunks, context, bedrock_client, model_id, prompt, do_formatting = True, cost_tracking = None):\n",
    "    results = []\n",
    "    \n",
    "    for chunk in text_chunks:\n",
    "        results.append(run_llm(chunk, context, bedrock_client, model_id, prompt, do_formatting, cost_tracking))\n",
    "        \n",
    "    return results\n",
    "\n",
    "def split_string_by_multi_markers(content, markers):\n",
    "    \"\"\"Split a string by multiple markers\"\"\"\n",
    "    if not markers:\n",
    "        return [content]\n",
    "    results = re.split(\"|\".join(re.escape(marker) for marker in markers), content)\n",
    "    return [r.strip() for r in results if r.strip()]\n",
    "def is_float_regex(value):\n",
    "    return bool(re.match(r\"^[-+]?[0-9]*\\.?[0-9]+$\", value))\n",
    "def capitalize(value):\n",
    "    if not isinstance(value, str):\n",
    "        return value\n",
    "    return value[0].upper()+value[1:] if len(value) > 1 else value.upper()\n",
    "def clean_str(input):\n",
    "    if not isinstance(input, str):\n",
    "        return input\n",
    "    result = html.unescape(input.strip())\n",
    "    result = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", result)\n",
    "    result = re.sub(r\"\\s+\", '_', result) \n",
    "    return result\n",
    "def _log_if_debug(info, debug):\n",
    "    if debug:\n",
    "        print(info)\n",
    "class Entity:\n",
    "    def __init__(self, identifier, label, name, description, source_chunk):\n",
    "        self.identifier = identifier\n",
    "        self.label = label\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.source_chunk = source_chunk\n",
    "    def __str__(self):\n",
    "        return f\"Entity({self.identifier}) [type={self.label},name={self.name},description={self.description},source_chunk={self.source_chunk}]\"\n",
    "    def to_array(self):\n",
    "        return [self.identifier, self.label, self.name, self.description]\n",
    "    def __json__(self):\n",
    "        return dict(\n",
    "            obj_class=\"Entity\",\n",
    "            identifier=self.identifier,\n",
    "            label=self.label,\n",
    "            description=self.description,\n",
    "            name=self.name,\n",
    "            source_chunk=self.source_chunk\n",
    "        )\n",
    "class Relationship:\n",
    "    def __init__(self, from_node, to_node, description, weight, keywords, source_chunk, from_id = None, to_id = None):\n",
    "        self.from_node = from_node\n",
    "        self.from_id = from_id\n",
    "        self.to_node = to_node\n",
    "        self.to_id = to_id\n",
    "        self.description = description\n",
    "        self.weight = weight\n",
    "        self.keywords = keywords\n",
    "        self.source_chunk = source_chunk\n",
    "    def __str__(self):\n",
    "        return f\"Relationship{{from={self.from_node}, to={self.to_node}}} [keywords={self.keywords},description={self.description},weight={self.weight},source_chunk={self.source_chunk}]\"\n",
    "    def __json__(self):\n",
    "        return dict(\n",
    "            obj_class=\"Relationship\",\n",
    "            from_node=self.from_node,\n",
    "            to_node=self.to_node,\n",
    "            description=self.description,\n",
    "            weight=self.weight,\n",
    "            keywords=self.keywords,\n",
    "            source_chunk=self.source_chunk,\n",
    "            from_id=self.from_id,\n",
    "            to_id=self.to_id\n",
    "        )\n",
    "def parse_entity(\n",
    "    record_attributes: list[str],\n",
    "    chunk_key: str,\n",
    "):\n",
    "    if len(record_attributes) < 4 or record_attributes[0] != '\"entity\"':\n",
    "        return None\n",
    "    proposed_id = f\"NODE_{clean_str(record_attributes[2].upper())}_{clean_str(record_attributes[1].upper())}\"\n",
    "    entity_name = record_attributes[1]\n",
    "    if not entity_name.strip():\n",
    "        return None\n",
    "    entity_type = clean_str(record_attributes[2].upper())\n",
    "    if not entity_type:\n",
    "        print(f\"WARNING: None entity type found for {str(record_attributes)}\")\n",
    "    entity_description = record_attributes[3]\n",
    "    entity_source_id = chunk_key\n",
    "    return Entity(\n",
    "        identifier = proposed_id,\n",
    "        label=entity_type,\n",
    "        name=entity_name,\n",
    "        description=entity_description,\n",
    "        source_chunk=entity_source_id\n",
    "    )\n",
    "\n",
    "def parse_relationship(\n",
    "    record_attributes: list[str],\n",
    "    chunk_key: str,\n",
    "):\n",
    "    if len(record_attributes) < 5 or record_attributes[0] != '\"relationship\"':\n",
    "        return None\n",
    "    source = record_attributes[1]\n",
    "    target = record_attributes[2]\n",
    "    edge_description = record_attributes[3]\n",
    "    edge_keywords = record_attributes[4]\n",
    "    edge_source_id = chunk_key\n",
    "    weight = (\n",
    "        float(record_attributes[-1]) if is_float_regex(record_attributes[-1]) else 1.0\n",
    "    )\n",
    "    return Relationship(\n",
    "        from_node=source, \n",
    "        to_node=target, \n",
    "        description=edge_description, \n",
    "        weight=weight,\n",
    "        keywords=edge_keywords,\n",
    "        source_chunk = chunk_key\n",
    "    )\n",
    "    \n",
    "def parse_question(text, context):\n",
    "    records = split_string_by_multi_markers(\n",
    "        response_text,\n",
    "        [context[\"record_delimiter\"], context[\"completion_delimiter\"]],\n",
    "    )\n",
    "    search_entities = []\n",
    "    search_type = None\n",
    "    search_keywords = []\n",
    "\n",
    "    for record in records:\n",
    "        record = re.search(r\"\\((.*)\\)\", record)\n",
    "        if record is None:\n",
    "            continue\n",
    "        record = record.group(1)\n",
    "    #    print(record)\n",
    "        record_attributes = split_string_by_multi_markers(\n",
    "            record, [context[\"tuple_delimiter\"]]\n",
    "        )\n",
    "    #    print(record_attributes)\n",
    "\n",
    "        match(record_attributes[0]):\n",
    "            case '\"entity\"':\n",
    "                search_entities.append(dict(name=record_attributes[1],label=record_attributes[2].upper()))\n",
    "            case '\"content_keywords\"':\n",
    "                search_keywords = record_attributes[1].split(\",\")\n",
    "            case '\"inquiry_type\"':\n",
    "                search_type = record_attributes[1]\n",
    "            case _:\n",
    "                print(f\"Unknown type: {record_attributes[0]}\")\n",
    "    return (search_entities, search_type, search_keywords)\n",
    "        \n",
    "def parse_llm_output(results, context, debug = False):\n",
    "    entities = []\n",
    "    relations = []\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        _log_if_debug(f\"~~record {str(idx)}:~~\",debug)\n",
    "        records = split_string_by_multi_markers(\n",
    "            result,\n",
    "            [context[\"record_delimiter\"], context[\"completion_delimiter\"]],\n",
    "        )\n",
    "        _log_if_debug(f\"{str(len(records))} items.\", debug)\n",
    "        local_str_to_id_lookup = {}\n",
    "        for record in records:\n",
    "            record = re.search(r\"\\((.*)\\)\", record)\n",
    "            if record is None:\n",
    "                continue\n",
    "            record = record.group(1)\n",
    "            _log_if_debug(record,debug)\n",
    "            record_attributes = split_string_by_multi_markers(\n",
    "                record, [context[\"tuple_delimiter\"]]\n",
    "            )\n",
    "            _log_if_debug(record_attributes,debug)\n",
    "        \n",
    "            entity = parse_entity(\n",
    "                record_attributes, f\"Chunk_{str(idx)}\"\n",
    "            )\n",
    "            if entity is not None:\n",
    "                _log_if_debug(f\"adding key {entity.name} -> {entity.identifier}\",debug)\n",
    "                local_str_to_id_lookup[entity.name] = entity.identifier\n",
    "                entities.append(entity)\n",
    "            \n",
    "            relation = parse_relationship(\n",
    "                record_attributes, f\"Chunk_{str(idx)}\"\n",
    "            )\n",
    "            if relation is not None:\n",
    "                relation.from_id = local_str_to_id_lookup[relation.from_node] if relation.from_node in local_str_to_id_lookup else None\n",
    "                if relation.from_id is None:\n",
    "                    print(f\"Source lookup error -- Cannot find {relation.from_node}. Dumping lookup dictionary:\")\n",
    "                    for key in local_str_to_id_lookup:\n",
    "                        print(f\"{key}->{local_str_to_id_lookup[key]}\")\n",
    "                relation.to_id = local_str_to_id_lookup[relation.to_node] if relation.to_node in local_str_to_id_lookup else None\n",
    "                if relation.to_id is None:\n",
    "                    print(f\"Target lookup error -- Cannot find {relation.to_node}. Dumping lookup dictionary:\")\n",
    "                    for key in local_str_to_id_lookup:\n",
    "                        print(f\"{key}->{local_str_to_id_lookup[key]}\")\n",
    "                relations.append(relation)\n",
    "    return (entities, relations)\n",
    "\n",
    "def format_as_neptune_load_files(text_chunks, entities, relations):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    # write a node for each chunk\n",
    "    chunk_label = \"Chunk\"\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        clean_chunk_text = re.sub('\\n', ' ', chunk)\n",
    "        nodes.append(\n",
    "            dict(\n",
    "                id = f\"NODE_CHUNK_{idx}\",\n",
    "                label = chunk_label,\n",
    "                name = f\"Chunk_{idx}\",\n",
    "                description = clean_chunk_text       \n",
    "            )\n",
    "        )\n",
    "\n",
    "    # convert objects to nodes and edges\n",
    "    for entity in entities:\n",
    "        # write the node\n",
    "        nodes.append(\n",
    "            dict(\n",
    "                id = entity.identifier,\n",
    "                label = entity.label,\n",
    "                name = entity.name,\n",
    "                description = entity.description       \n",
    "            )\n",
    "        )\n",
    "        # write the edge to the source chunk\n",
    "        edges.append(\n",
    "            dict(\n",
    "                from_id=entity.identifier,\n",
    "                to_id=f\"NODE_{entity.source_chunk.upper()}\",\n",
    "                label=\"hasChunk\",\n",
    "                description=\"\",\n",
    "                weight=1.0,\n",
    "                keywords=\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for relation in relations:\n",
    "        labels = relation.keywords.split(\",\")\n",
    "\n",
    "        for label in labels:\n",
    "            trimmed_label = label.strip()\n",
    "            edges.append(\n",
    "                dict(\n",
    "                    from_id=relation.from_id,\n",
    "                    to_id=relation.to_id,\n",
    "                    label=trimmed_label[0].upper()+trimmed_label[1:],\n",
    "                    description=relation.description,\n",
    "                    weight=relation.weight,\n",
    "                    keywords=relation.keywords\n",
    "                )\n",
    "            )\n",
    "    return (nodes, edges)\n",
    "\n",
    "def get_neptune_query_template(question_type):\n",
    "    return connections_template if question_type == \"Connections\" else inquiry_template\n",
    "\n",
    "def query_facts_from_neptune(search_entities, query_template):\n",
    "    if (len(search_entities) > 2):\n",
    "        print(\"WARNING: Found more than two entities.  This current version will only use the first two entities in the graph query\")\n",
    "    if (search_type == \"Connections\" and len(search_entities) < 2):\n",
    "        print(\"ERROR: Cannot execute a Connections query with only a single entity.  Something went wrong.\")\n",
    "        return\n",
    "    if (len(search_entities) <= 0):\n",
    "        print(\"ERROR: No entities were extracted for the query. Something went wrong.\")\n",
    "        return\n",
    "    \n",
    "    parameters = {}\n",
    "\n",
    "    if (len(search_entities) >= 1):\n",
    "        if len(search_entities) >= 2:\n",
    "            parameters[\"entity2_text\"] = search_entities[1][\"name\"]\n",
    "            parameters[\"entity2_label\"] = search_entities[1][\"label\"]\n",
    "        parameters[\"entity1_text\"] = search_entities[0][\"name\"]\n",
    "        parameters[\"entity1_label\"] = search_entities[0][\"label\"]\n",
    "\n",
    "\n",
    "#        print(query_template)\n",
    "#        print(parameters)\n",
    "\n",
    "        neptune_response = neptune_client.execute_query(\n",
    "            graphIdentifier = json.loads(config)['host'].split(\".\")[0],\n",
    "            queryString=query_template,\n",
    "            parameters=parameters,\n",
    "            language='OPEN_CYPHER'\n",
    "        )\n",
    "    else:\n",
    "        print(\"No entities found in question. Cannot continue.\")\n",
    "        return\n",
    "    \n",
    "    neptune_payload = json.load(neptune_response[\"payload\"])\n",
    "    facts = []\n",
    "\n",
    "    for row in neptune_payload[\"results\"]:\n",
    "        facts.append(row)\n",
    "    \n",
    "    answer_json = dict(facts=facts)\n",
    "    return answer_json\n",
    "\n",
    "def query_chunks_from_graph(search_entities, debug=False):\n",
    "    chunks = []\n",
    "    parameters = {}\n",
    "    parameters[\"entity1_text\"] = search_entities[0][\"name\"]\n",
    "    parameters[\"entity1_label\"] = search_entities[0][\"label\"]\n",
    "    neptune_response = neptune_client.execute_query(\n",
    "        graphIdentifier = json.loads(config)['host'].split(\".\")[0],\n",
    "        queryString=chunk_retrieval_template,\n",
    "        parameters=parameters,\n",
    "        language='OPEN_CYPHER'\n",
    "    )\n",
    "    neptune_payload = json.load(neptune_response[\"payload\"])\n",
    "    _log_if_debug(f\"Query Response is {neptune_payload}\",debug)\n",
    "    \n",
    "    for row in neptune_payload[\"results\"]:\n",
    "        chunks.append(row[\"text_chunk\"])\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "def calculate_embeddings_batch(text_chunks, bedrock_client,cost_tracking=None):\n",
    "    embeddings = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        embeddings.append(calculate_embedding(chunk, bedrock_client,cost_tracking))\n",
    "\n",
    "    return pd.DataFrame(embeddings, columns=['Text','Embeddings'])\n",
    "\n",
    "\n",
    "def calculate_embedding(chunk, bedrock_client, cost_tracking=None):\n",
    "    native_request = {\"inputText\": chunk}\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    response = bedrock_client.invoke_model(modelId=EMBEDDING_MODEL_ID, body=request)\n",
    "\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    embedding = model_response[\"embedding\"]\n",
    "    if not (cost_tracking is None):\n",
    "        cost_tracking[\"prompt_tokens\"] = cost_tracking[\"prompt_tokens\"] + model_response[\"inputTextTokenCount\"] if \"prompt_tokens\" in cost_tracking else response_body[\"prompt_token_count\"]\n",
    "        cost_tracking[\"generation_tokens\"] = cost_tracking[\"generation_tokens\"] + 0 if \"generation_tokens\" in cost_tracking else 0\n",
    "\n",
    "    return (chunk, np.array(embedding))\n",
    "\n",
    "def calculate_similarity_to_question(corpus, question_embedding):\n",
    "    return cosine_similarity(question_embedding[1].reshape(1,-1), np.array(corpus[\"Embeddings\"].tolist()))\n",
    "    \n",
    "def print_vectors_side_by_side(vector1, vector2):\n",
    "\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    side_by_side = (vector1.reshape(1,-1),vector2.reshape(1,-1))\n",
    "    for i in range(0,len(side_by_side[0][0])):\n",
    "        print(f\"dim: {i} {side_by_side[0][0][i]} <-> {side_by_side[1][0][i]}\")\n",
    "def rag_query(question, text_chunks, bedrock_client,model_id):\n",
    "    dataframe_embeddings = calculate_embeddings_batch(text_chunks, bedrock_client)\n",
    "\n",
    "    question_embedding = calculate_embedding(question, bedrock_client)\n",
    "    \n",
    "    rag_similarity = calculate_similarity_to_question(dataframe_embeddings, question_embedding)\n",
    "    dataframe_embeddings[\"similarity\"] = rag_similarity.reshape(-1,1)\n",
    "\n",
    "    dataframe_ordered = dataframe_embeddings.nlargest(3, 'similarity')\n",
    "    context[\"additionalInfo\"] = \"\\n\".join((dataframe_ordered[\"Text\"].iloc[0],dataframe_ordered[\"Text\"].iloc[1]))\n",
    "    context[\"question\"] = question\n",
    "\n",
    "    return run_llm(question,context, bedrock_client, model_id, PROMPTS[\"RAG_QUESTION\"])\n",
    "def generate_summary(company1, company2, brand1, product1, country2, company4, person_name):\n",
    "\n",
    "    print(f\"\"\"\n",
    "    > {company2} acquired {company1} including their brand {brand1} for cash.  It also talks about their business and related charges.\n",
    "    > {capitalize(product1)} had some difficulties in their business model and supply chain during the pandemic.  They shifted to a new supplier in {country2} quickly so they could continue production.\n",
    "    > {company2} declared bankruptcy and is selling itself to rival {company4}.  Their CEO {person_name} believes this will benefit all parties.\n",
    "    > {person_name} was arrested in {country2} for bribing a government official during the pandemic to help overcome some supply chain woes.\n",
    "    \"\"\")\n",
    "\n",
    "PROMPTS = {}\n",
    "\n",
    "PROMPTS[\"ENTITY_EXTRACTION\"] = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "-Goal-\n",
    "Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: One of the following types: [{{entity_types}}]\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>\n",
    "Use **{record_delimiter}** as the delimiter after each entity\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "Your goal is to extract 10 or more relationships from this document. If the relationship's entities were not already identified in step 1, please add them.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details\n",
    "- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity. A score of 1.0 represents the highest strength and a score of 0.0 represents the lowest strength.\n",
    "Format each relationship as (\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)\n",
    "Use **{record_delimiter}** as the delimiter after each relationship\n",
    "\n",
    "3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\n",
    "Format the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_level_keywords>)\n",
    "\n",
    "4. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2.\n",
    "5. When finished, output {completion_delimiter}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Entity_types: {entity_types}\n",
    "Text: {input_text}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPTS[\"QUESTION_EXTRACTION\"] = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "-Goal-\n",
    "Given a text prompt from a user looking for information and a list of entity types, identify all entities of those types from the text, all relationships among the identified entities, and the type of question being asked.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: One of the following types: [{{entity_types}}]\n",
    "Format each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>\n",
    "Use **{record_delimiter}** as the delimiter after each entity\n",
    "\n",
    "2. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\n",
    "Format the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_level_keywords>)\n",
    "Use **{record_delimiter}** as the delimiter after each entity\n",
    "\n",
    "3. Identify what type of inquiry the customer is making. Here are the guidelines to use:\n",
    "If the question has two entities and is in a format like \"What is the connection between entity1 and entity2?\" then it is a \"Connections\" inquiry.\n",
    "If the question asks for information about a single entity or event then it is an \"Information\" inquiry.\n",
    "If you aren't sure what type of question it is, then choose an \"Information\" inquiry.\n",
    "Format the inquiry as (\"inquiry_type\"{tuple_delimiter}<inquiry_type>)\n",
    "Use **{record_delimiter}** as the delimiter after each entity\n",
    "\n",
    "4. Return output in English as a single list of all the entities, key words, and inquiry type identified in steps 1, 2, and 3.\n",
    "\n",
    "5. When finished, output {completion_delimiter}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Entity_types: {entity_types}\n",
    "Text: {input_text}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPTS[\"RAG_QUESTION\"] = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are excellent at answering questions, and it makes you happy when you provide the correct answer.\n",
    "Consider the additional information when creating your response.  Create a narrative paragraph answering the question below. After the narrative paragraph, list any relevant facts \n",
    "you used to answer as a bulleted list\n",
    "Additional Information:\n",
    "{additionalInfo}\n",
    "Question:\n",
    "{question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177926b",
   "metadata": {},
   "source": [
    "# Here are the values we will be substituting into our stories. \n",
    "\n",
    "Feel free to substitute your own values as desired. Please do not abuse this to create false or misleading examples by using real names.  Run the cell when you have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04fb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Date, 2 years, a month, and a period of time (with units) respectively (4 different values, not forms of the same date)\n",
    "date1 = \"April 1, 1977\"\n",
    "year1 = \"1977\"\n",
    "year2 = \"1952\"\n",
    "month1 = \"June\"\n",
    "period_of_time1 = \"17 millenia\"\n",
    "\n",
    "# Some company names (fictional)\n",
    "company1 = \"King Kong Brands\"\n",
    "company2 = \"Purple Ventures\" \n",
    "company3 = \"Intergalactic Management\"\n",
    "company4 = \"Loads'o'Money Corp.\"\n",
    "company5 = \"BrianCorp\"\n",
    "\n",
    "# Some products a company might sell\n",
    "product1 = \"pickled beets\"\n",
    "product2 = \"orangutan cages\"\n",
    "product3 = \"deviled eggs\"\n",
    "\n",
    "# A fictional brand name\n",
    "brand1 = \"Snippits\"   \n",
    "\n",
    "# Some monetary amounts (preferrably with currency)\n",
    "amount1 = \"$17 billion\" \n",
    "amount2 = \"15.67 euros\"\n",
    "amount3 = \"$100 million\"\n",
    "amount4 = \"17 gold dubloons\"\n",
    "amount5 = \"74 shekels\"\n",
    "\n",
    "# a business segment\n",
    "business_segment1 = \"nails and fasteners\"\n",
    "\n",
    "# a country name (real or fictional)\n",
    "country1 = \"Wakanda\"\n",
    "country2 = \"Atlantis\"\n",
    "\n",
    "# a disease (real or fictional)\n",
    "disease1 = \"Smallpox\"\n",
    "\n",
    "# a city (real or fictional)\n",
    "city1 = \"Smorgasbord\"\n",
    "\n",
    "# a state (real or fictional)\n",
    "state1 = \"New Florida\"\n",
    "\n",
    "# a person's name (fictional)\n",
    "person_name = \"Rick Roll\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3414ccb",
   "metadata": {},
   "source": [
    "### Here we are generating 4 fictional financial \"news\" stories substituting the phrases above.\n",
    "\n",
    "Let's take a quick look at those stories after the phrases are substituted, or skip to the next cell for the `Too Long; Didn't Read` (TL;DR) version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce342f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [\n",
    "f\"\"\"On {date1}, we ({company2}) completed the acquisition of {company1}, a leader in the production and co-packing of {product1} and ready-to-eat {product2}, and former co-manufacturer of the {brand1} brand. The initial cash consideration paid for {company1} totaled {amount1} and consisted of cash on hand and short-term borrowings. Acquisition-related costs for the {company1} acquisition were immaterial.\n",
    "The acquisition has been accounted for as a business combination and, accordingly, {company1} has been included within the {business_segment1} segment from the date of acquisition. The purchase consideration was allocated to assets acquired and liabilities assumed based on their respective fair values and consisted of {amount2} to goodwill, {amount3} to property, plant and equipment, net and {amount4} to other net assets acquired. The purchase price allocation has been finalized as of the fourth quarter of {year1} and did not include measurement period adjustments.\n",
    "Goodwill was determined as the excess of the purchase price over the fair value of the net assets acquired. The goodwill derived from this acquisition is deductible for tax purposes and reflects the value of leveraging our supply chain capabilities to accelerate growth and access to our portfolio of {product1} products\"\"\",\n",
    "f\"\"\"{capitalize(product1)} companies such as {company1} had to flip from sending bulk volumes to schools and restaurants to feeding people working from home who suddenly had time for breakfast. Finding enough paperboard packaging for {product1} became a constraint.\n",
    "With families staying home and limiting supermarket trips, the pandemic boosted sales of {company1}'s {product1}, {product2}, and {product3}. \n",
    "For the multinational, food consumed at home more than offset declines in on-the-go channels. While growth had moderated by {month1}, {company1}’s sales over the first nine months of {year1} increased 7% over the year-ago period, to {amount5}, excluding the effects of divestitures and currency rate fluctuations.\n",
    "Packaging became a bottleneck, as the region’s {country1} supplier, {company3}, ran short due to shipping delays caused by the {disease1} pandemic. The procurement team “scoured the world” for a new source of paperboard and found an alternative supplier next door in {country2}. Lower transportation costs helped to make up for the higher cost of the {country2} paperboard.\"\"\",\n",
    "f\"\"\"{capitalize(company2)}, a national {product1} retailer whose roots go back more than {period_of_time1}, said Monday that it has declared bankruptcy and will sell itself to a competitor. \n",
    "The {city1}-based company filed for Chapter 11 protection from its debts in the U.S. Bankruptcy Court for the District of {state1}. As part of the filing, most of the privately held retailer's assets will be acquired by {business_segment1} rival {company4}. {company2}, which was founded in {year2}, said it will continue providing independently owned retailers with products.\n",
    "\"We believe that entering the process with an agreed offer from {company4}, who has a similar {period_of_time1} history in the {business_segment1} space and also operates with a focus on supporting members and helping them grow, is the most beneficial next step for {company2} and our associates, customers and vendor partners,\" {company2} CEO {person_name} said in a statement.\"\"\",\n",
    "f\"\"\"The CEO of a prominent {business_segment1} firm was arrested in {country2} today on charges of bribing a government official. The charges date back to the supply chain woes during the {disease1} pandemic. {person_name} will make his initial appearance in court later this week. The markets were roiled by the fear of what effect this will have on his company and the bankruptcy rumors surrounding it.\"\"\"\n",
    "]\n",
    "\n",
    "pprint(json.dumps(text_chunks, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392009a2",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "Run this function to get a summary of the key facts that will be relevant later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6deef9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_summary(company1, company2, brand1, product1, country2, company4, person_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086160a",
   "metadata": {},
   "source": [
    "## Let's choose a version of Meta Llama 3.2 to use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are US-based inference endpoints.  Change them accordingly for your region.\n",
    "class SupportedModels(Enum):\n",
    "    META_LLAMA32_90B = \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    META_LLAMA32_11B = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "    META_LLAMA32_1B  = \"us.meta.llama3-2-1b-instruct-v1:0\"\n",
    "    META_LLAMA32_3B  = \"us.meta.llama3-2-3b-instruct-v1:0\"\n",
    "    \n",
    "model_id = SupportedModels.META_LLAMA32_90B.value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee66f0b",
   "metadata": {},
   "source": [
    "### Next let's ask a question of the LLM using traditional RAG methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5574a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = f\"Did any personal events contribute to the downfall of {company2}?\"\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2592bbc",
   "metadata": {},
   "source": [
    "### The RAG Approach\n",
    "\n",
    "RAG involves: \n",
    "1. taking chunks of a document and encoding them as a representative vector of numbers.  \n",
    "2. Then the question being asked is also encoded as a vector of numbers.\n",
    "3. Next, we retrieve a number of the chunks (1) that are most similar to the question (2) using fancy math called cosine similarity.\n",
    "4. Finally, we add the chunks of the documents into the LLM prompt with the question so it has more context to answer.\n",
    "\n",
    "Let's see this in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c47884",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_dataframe = calculate_embeddings_batch(text_chunks, bedrock_client)\n",
    "\n",
    "question_embedding = calculate_embedding(question, bedrock_client)\n",
    "\n",
    "similarity = calculate_similarity_to_question(chunks_dataframe, question_embedding)\n",
    "chunks_dataframe[\"similarity\"] = similarity.reshape(-1,1)\n",
    "\n",
    "ordered_chunks = chunks_dataframe.nlargest(4, 'similarity')\n",
    "\n",
    "ordered_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993ac65",
   "metadata": {},
   "source": [
    "You can see above the 4 stories, or chunks, ordered by the similarity to our question. One of the challenges is figuring out where the cutoff is between similar enough and not. We'll take the top 50% here, but that won't scale well for a large set of documents, so you may need to adjust accordingly.  So let's take those top 2 documents and inject them into the prompt with our question and let Llama give us an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36292c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "context[\"additionalInfo\"] = \"\\n\".join((ordered_chunks[\"Text\"].iloc[0],ordered_chunks[\"Text\"].iloc[1]))\n",
    "context[\"question\"] = question\n",
    "\n",
    "rag_answer = run_llm(question,context, bedrock_client, model_id, PROMPTS[\"RAG_QUESTION\"])\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b76d5",
   "metadata": {},
   "source": [
    "### What do you think of the answer? \n",
    "\n",
    "The answer should feel well researched, but it is missing important information if you read the documents closely. Regardless, it would be interesting to see why it was determined those first two documents are so relevant to the question. Let's take a peak behind the curtain at the similarity between the question and just the most relevant document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567bb12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_vectors_side_by_side(ordered_chunks[\"Embeddings\"].iloc[0], question_embedding[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3ddd7",
   "metadata": {},
   "source": [
    "### Uhhh...\n",
    "\n",
    "I really wish each of those dimensions came with an explanation.  I'm not giving this high marks on traceability and explainability.\n",
    "\n",
    "Now let's take a look at what GraphRAG is.\n",
    "\n",
    "### GraphRAG\n",
    "\n",
    "GraphRAG is a little different than RAG.  It involves: \n",
    "1. Asking an LLM to extract entities and relationships connecting those entities from each text chunk, instead of just encoding the entire thing as numbers.\n",
    "2. Those entities (nodes) and relationships (edges) are then loaded into a graph.\n",
    "3. Then the LLM is asked to extract the entities from the question as well.\n",
    "4. Then we run a query in the graph database to get back all of the entities and relationships that are relevant to the entities from (3) \n",
    "5. Finally, we add the relevant entities and relationships into the LLM prompt with the question so it has more context to answer.\n",
    "\n",
    "Let's see this in action...first, let's do the one-time steps to load all of our documents into our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the types of entities we want the LLM to extract\n",
    "context[\"entity_types\"] = [\"organization\",\"person\",\"location\",\"monetary_value\",\"product\",\"date\"]\n",
    "\n",
    "# Send each of the documents to the LLM (step 1)\n",
    "results = run_llm_batch(text_chunks, context, bedrock_client, model_id, PROMPTS[\"ENTITY_EXTRACTION\"])\n",
    "\n",
    "# Get back the entities and results and convert them into nodes and edges (starting step 2)\n",
    "(entities, relations) = parse_llm_output(results, context)\n",
    "(nodes, edges) = format_as_neptune_load_files(text_chunks, entities, relations)\n",
    "\n",
    "# write CSV files to load into Neptune\n",
    "with open('graph_rag_demo_nodes.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([':ID',':LABEL','name:String','description:String'])\n",
    "    for node in nodes:\n",
    "        writer.writerow([node[\"id\"], node[\"label\"], node[\"name\"], node[\"description\"]])\n",
    "\n",
    "with open('graph_rag_demo_edges.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([':ID',':START_ID',':END_ID',':TYPE', 'description:String','weight:Float','keywords:String'])\n",
    "    for edge in edges:\n",
    "        writer.writerow([str(uuid.uuid4()), edge[\"from_id\"],edge[\"to_id\"],edge[\"label\"],edge[\"description\"],edge[\"weight\"],edge[\"keywords\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596385fd",
   "metadata": {},
   "source": [
    "### Copy those files into S3 so Neptune can access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {s3_bucket}\n",
    "\n",
    "aws s3 cp ./graph_rag_demo_nodes.csv $1\n",
    "aws s3 cp ./graph_rag_demo_edges.csv $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03cd7a",
   "metadata": {},
   "source": [
    "### Load the files into Neptune\n",
    "\n",
    "Again, substitute your S3 bucket name below where it says `$BUCKET-NAME-HERE$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877010c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%oc \n",
    "\n",
    "CALL neptune.load({format: \"opencypher\", \n",
    "                   source: \"s3://$BUCKET-NAME-HERE$\", \n",
    "                   region : \"us-east-1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ef800",
   "metadata": {},
   "source": [
    "### Let's take a look at what this graph looks like\n",
    "\n",
    "I think you'll find this much more understandable than the RAG comparison.  Click on the Graph tab in the results window and take a look at our nodes and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07af03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%oc -d name -l 50 -rel 50\n",
    "\n",
    "MATCH p=(n)-[]-(n1)\n",
    "RETURN p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6255cb",
   "metadata": {},
   "source": [
    "## Now let's run our GraphRAG query\n",
    "\n",
    "Again, we are asking the LLM to extract the entity types below from our question, we find those nodes in our graph along with those related to it, and we add all this information into our LLM prompt and ask it to create a narrative paragraph summarizing the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ba907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context[\"entity_types\"] = [\"organization\",\"person\",\"location\",\"monetary_value\",\"product\",\"date\"]\n",
    "\n",
    "response_text = run_llm(question, context, bedrock_client, model_id, PROMPTS[\"QUESTION_EXTRACTION\"])\n",
    "(search_entities, search_type, search_keywords) = parse_question(response_text, context)\n",
    "\n",
    "query_template = get_neptune_query_template(search_type)\n",
    "facts = query_facts_from_neptune(search_entities, query_template)\n",
    "\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are excellent at answering questions, and it makes you happy when you provide the correct answer.\n",
    "Consider the list of facts supplied in JSON format when creating your response.  Create a narrative paragraph answering the question below. After the narrative paragraph, list any relevant facts \n",
    "you used to answer as a bulleted list\n",
    "Facts:\n",
    "{json.dumps(facts)}\n",
    "Question:\n",
    "{question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "graphrag_answer = run_llm(question,context, bedrock_client, model_id, prompt, do_formatting=False)\n",
    "print(graphrag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca6104",
   "metadata": {},
   "source": [
    "### So let's compare the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296bcfc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"RAG Answer:\\n\")\n",
    "print(rag_answer)\n",
    "print(\"\\n--------------\\n\")\n",
    "print(\"GraphRAG Answer:\")\n",
    "print(graphrag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ef46f",
   "metadata": {},
   "source": [
    "### Key things to notice:\n",
    "1. The RAG answer has more \"filler\" words and detail because we are passing the entire document into the prompt, not just the facts.  This is especially important here because everything is fabricated, so it cannot draw upon its own training data to add color.\n",
    "\n",
    "2. The GraphRAG answer includes details that don't seem relevant when looking at just the wording of the question.  For example, the CEO of the company was arrested for bribery in one of the countries where it does business.  If we look back at that news story, it doesn't mention the company at all and therefore had a least relevant score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645edca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_params = {\"entity_name\": person_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0565ff",
   "metadata": {},
   "source": [
    "### Which chunks did it use exactly?  \n",
    "\n",
    "The easier traceability and explainability lets us easily identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736144f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%oc -d name -l 50 -rel 50 -qp query_params\n",
    "\n",
    "MATCH p=(entity1:PERSON)-[event*0..1]-(entity2)-[event2:hasChunk]->(chunk:Chunk)\n",
    "WHERE $entity_name = entity1.name\n",
    "RETURN p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cacc67",
   "metadata": {},
   "source": [
    "### Can we combine them?\n",
    "\n",
    "Certainly.  Here we are capturing all of the chunks associated to the relevant entities in our graph. Is the answer better or does the presence of too much information lead the LLM astray...I think that is up to the reader's interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac268b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_text = \"\\n\".join(query_chunks_from_graph(search_entities))\n",
    "\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are excellent at answering questions, and it makes you happy when you provide the correct answer.\n",
    "Consider both the list of facts supplied in JSON format when creating your response, as well as text in the additional information section.  Create a narrative paragraph answering the question below. After the narrative paragraph, list any relevant facts \n",
    "you used to answer as a bulleted list\n",
    "Facts:\n",
    "{json.dumps(facts)}\n",
    "Raw text:\n",
    "{raw_text}\n",
    "Question:\n",
    "{question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "blended_answer = run_llm(question,context, bedrock_client, model_id, prompt, do_formatting=False)\n",
    "print(blended_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeac421",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">Please remember to delete your Neptune Analytics graph, remove the files from the S3 bucket, and remove your Neptune Notebook instance so you do not have recurring charges from this demonstration.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174482a",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we demonstrated a scenario where GraphRAG gave a better answer than standard RAG. It is important to recognize that not every scenario will have the same result. If the wording of the last news story or the question was different (e.g., the story mentioned the company the CEO worked for, or the question mentioned the CEO by name), then RAG would likely have picked up on the arrest as well. It is also important to recognize the inherent nature of large language models (LLMs) being probabilistic and trained on a general corpus of words means that it is entirely possible to insert words that the LLM won't recognize as an entity and therefore the results will not be meaningful. Finally, note that running Graph RAG scenarios is significantly more expensive today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
